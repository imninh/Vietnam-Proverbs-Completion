import re
import sys
import unicodedata
from pathlib import Path
from collections import Counter

sys.path.append(str(Path(__file__).parent.parent))


class DatasetCleaner:
    """Class x·ª≠ l√Ω l√†m s·∫°ch dataset ca dao"""
    
    def __init__(self):
        self.stats = {
            'original': 0,
            'empty_removed': 0,
            'short_removed': 0,
            'long_removed': 0,
            'invalid_removed': 0,
            'duplicate_removed': 0,
            'final': 0
        }
        self.removed_samples = {
            'empty': [],
            'short': [],
            'long': [],
            'invalid': [],
            'duplicate': []
        }
    
    # ========== B∆Ø·ªöC 1: X√ìA D√íNG R·ªñNG ==========
    def remove_empty_lines(self, lines):
        """X√≥a d√≤ng tr·ªëng v√† ch·ªâ c√≥ kho·∫£ng tr·∫Øng"""
        cleaned = []
        for line in lines:
            stripped = line.strip()
            if stripped:
                cleaned.append(stripped)
            else:
                self.stats['empty_removed'] += 1
                self.removed_samples['empty'].append(line)
        
        return cleaned
    
    # ========== B∆Ø·ªöC 2: CHU·∫®N H√ìA UNICODE ==========
    def normalize_unicode(self, text):
        """
        Chu·∫©n h√≥a d·∫•u ti·∫øng Vi·ªát (NFC normalization)
        VD: '√°' c√≥ th·ªÉ l√† 1 k√Ω t·ª± ho·∫∑c 'a' + d·∫•u s·∫Øc ‚Üí chu·∫©n h√≥a v·ªÅ 1 d·∫°ng
        """
        return unicodedata.normalize('NFC', text)
    
    # ========== B∆Ø·ªöC 3: X√ìA K√ù T·ª∞ KH√îNG H·ª¢P L·ªÜ ==========
    def clean_text(self, text):
        """
        X√≥a s·ªë, k√Ω t·ª± ƒë·∫∑c bi·ªát, gi·ªØ l·∫°i ch·ªØ c√°i ti·∫øng Vi·ªát
        """
        original = text
        
        # 1. X√≥a s·ªë ƒë·∫ßu d√≤ng (VD: "1. ƒÇn qu·∫£..." ‚Üí "ƒÇn qu·∫£...")
        text = re.sub(r'^\d+[\.\):\s]+', '', text)
        
        # 2. X√≥a d·∫•u ngo·∫∑c, g·∫°ch ngang
        text = re.sub(r'[\(\)\[\]{}\-‚Äì‚Äî_]', ' ', text)
        
        # 3. Gi·ªØ ch·ªØ c√°i, s·ªë, d·∫•u ph·∫©y, ch·∫•m, kho·∫£ng tr·∫Øng
        text = re.sub(r'[^\w\s,\.]', '', text)
        
        # 4. X√≥a s·ªë (n·∫øu c√≤n)
        text = re.sub(r'\d+', '', text)
        
        # 5. X√≥a d·∫•u c√¢u ƒë·∫ßu/cu·ªëi
        text = text.strip(',. ')
        
        # 6. Chu·∫©n h√≥a kho·∫£ng tr·∫Øng (nhi·ªÅu space ‚Üí 1 space)
        text = ' '.join(text.split())
        
        return text
    
    # ========== B∆Ø·ªöC 4: CHUY·ªÇN CH·ªÆ TH∆Ø·ªúNG ==========
    def normalize_case(self, text):
        """
        Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
        Ca dao/t·ª•c ng·ªØ th∆∞·ªùng kh√¥ng c√≥ t√™n ri√™ng n√™n an to√†n
        """
        return text.lower()
    
    # ========== B∆Ø·ªöC 5: KI·ªÇM TRA H·ª¢P L·ªÜ ==========
    def is_valid_proverb(self, text):
        """
        Ki·ªÉm tra c√¢u c√≥ ph·∫£i ca dao/t·ª•c ng·ªØ h·ª£p l·ªá kh√¥ng
        
        Ti√™u ch√≠:
        - C√≥ √≠t nh·∫•t 50% ch·ªØ c√°i ti·∫øng Vi·ªát
        - Kh√¥ng ch·ª©a URL, email
        - Kh√¥ng to√†n k√Ω t·ª± ƒë·∫∑c bi·ªát
        """
        if not text or len(text.strip()) == 0:
            return False, "r·ªóng"
        
        # Ki·ªÉm tra URL
        if re.search(r'http[s]?://|www\.', text):
            return False, "c√≥ URL"
        
        # Ki·ªÉm tra email
        if re.search(r'\S+@\S+\.\S+', text):
            return False, "c√≥ email"
        
        # Ki·ªÉm tra t·ª∑ l·ªá ch·ªØ c√°i ti·∫øng Vi·ªát
        vietnamese_letters = re.findall(
            r'[a-z√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë]', 
            text.lower()
        )
        
        text_without_space = text.replace(' ', '').replace(',', '').replace('.', '')
        if len(text_without_space) == 0:
            return False, "ch·ªâ c√≥ kho·∫£ng tr·∫Øng"
        
        letter_ratio = len(vietnamese_letters) / len(text_without_space)
        
        if letter_ratio < 0.5:
            return False, f"√≠t ch·ªØ c√°i ({letter_ratio:.0%})"
        
        return True, None
    
    # ========== B∆Ø·ªöC 6: L·ªåC THEO ƒê·ªò D√ÄI ==========
    def filter_by_length(self, text, min_words=4, max_words=30):
        """
        Ki·ªÉm tra ƒë·ªô d√†i h·ª£p l·ªá
        Ca dao th∆∞·ªùng 5-15 t·ª´
        """
        words = text.split()
        word_count = len(words)
        
        if word_count < min_words:
            return False, f"qu√° ng·∫Øn ({word_count} t·ª´)"
        
        if word_count > max_words:
            return False, f"qu√° d√†i ({word_count} t·ª´)"
        
        return True, None
    
    # ========== PIPELINE CH√çNH ==========
    def clean(self, input_file, output_file, min_words=4, max_words=30):
        """Ch·∫°y to√†n b·ªô pipeline l√†m s·∫°ch"""
        
        print("\n" + "="*70)
        print("üßπ L√ÄM S·∫†CH DATASET CA DAO")
        print("="*70)
        
        # ƒê·ªçc file
        print(f"\nüìÅ ƒê·ªçc file: {input_file}")
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except FileNotFoundError:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {input_file}")
            return False
        
        self.stats['original'] = len(lines)
        print(f"‚úì ƒê·ªçc th√†nh c√¥ng {self.stats['original']:,} d√≤ng")
        
        # ========== STEP 1: X√≥a d√≤ng r·ªóng ==========
        print(f"\n{'‚îÄ'*70}")
        print("B∆Ø·ªöC 1: X√≥a d√≤ng r·ªóng")
        lines = self.remove_empty_lines(lines)
        print(f"‚úì X√≥a {self.stats['empty_removed']:,} d√≤ng r·ªóng")
        print(f"  C√≤n l·∫°i: {len(lines):,} d√≤ng")
        
        # ========== STEP 2-5: X·ª≠ l√Ω t·ª´ng d√≤ng ==========
        print(f"\n{'‚îÄ'*70}")
        print("B∆Ø·ªöC 2-5: Chu·∫©n h√≥a vƒÉn b·∫£n")
        
        cleaned_lines = []
        
        for line in lines:
            # 2. Chu·∫©n h√≥a Unicode
            line = self.normalize_unicode(line)
            
            # 3. X√≥a k√Ω t·ª± kh√¥ng h·ª£p l·ªá
            line = self.clean_text(line)
            
            # 4. Chuy·ªÉn ch·ªØ th∆∞·ªùng
            line = self.normalize_case(line)
            
            # 5. Ki·ªÉm tra h·ª£p l·ªá
            is_valid, reason = self.is_valid_proverb(line)
            if not is_valid:
                self.stats['invalid_removed'] += 1
                self.removed_samples['invalid'].append((line, reason))
                continue
            
            # 6. Ki·ªÉm tra ƒë·ªô d√†i
            length_ok, reason = self.filter_by_length(line, min_words, max_words)
            if not length_ok:
                if "ng·∫Øn" in reason:
                    self.stats['short_removed'] += 1
                    self.removed_samples['short'].append((line, reason))
                else:
                    self.stats['long_removed'] += 1
                    self.removed_samples['long'].append((line, reason))
                continue
            
            cleaned_lines.append(line)
        
        print(f"‚úì X√≥a {self.stats['invalid_removed']:,} c√¢u kh√¥ng h·ª£p l·ªá")
        print(f"‚úì X√≥a {self.stats['short_removed']:,} c√¢u qu√° ng·∫Øn")
        print(f"‚úì X√≥a {self.stats['long_removed']:,} c√¢u qu√° d√†i")
        print(f"  C√≤n l·∫°i: {len(cleaned_lines):,} c√¢u")
        
        # ========== STEP 7: X√≥a tr√πng l·∫∑p ==========
        print(f"\n{'‚îÄ'*70}")
        print("B∆Ø·ªöC 6: X√≥a tr√πng l·∫∑p")
        
        seen = set()
        unique_lines = []
        
        for line in cleaned_lines:
            if line not in seen:
                seen.add(line)
                unique_lines.append(line)
            else:
                self.stats['duplicate_removed'] += 1
                self.removed_samples['duplicate'].append(line)
        
        print(f"‚úì X√≥a {self.stats['duplicate_removed']:,} c√¢u tr√πng l·∫∑p")
        print(f"  C√≤n l·∫°i: {len(unique_lines):,} c√¢u")
        
        self.stats['final'] = len(unique_lines)
        
        # ========== L∆ØU K·∫æT QU·∫¢ ==========
        print(f"\n{'‚îÄ'*70}")
        print("B∆Ø·ªöC 7: L∆∞u k·∫øt qu·∫£")
        
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for line in unique_lines:
                f.write(line + '\n')
        
        print(f"‚úì ƒê√£ l∆∞u: {output_file}")
        
        # ========== B√ÅO C√ÅO CU·ªêI C√ôNG ==========
        self.print_report()
        
        # ========== M·∫™U D·ªÆ LI·ªÜU ==========
        self.print_samples(unique_lines)
        
        return True
    
    def print_report(self):
        """In b√°o c√°o t·ªïng h·ª£p"""
        print(f"\n{'='*70}")
        print("üìä B√ÅO C√ÅO T·ªîNG H·ª¢P")
        print("="*70)
        
        print(f"\nüìà Th·ªëng k√™:")
        print(f"   D√≤ng g·ªëc:              {self.stats['original']:>6,}")
        print(f"   ‚îú‚îÄ X√≥a r·ªóng:           {self.stats['empty_removed']:>6,}")
        print(f"   ‚îú‚îÄ X√≥a kh√¥ng h·ª£p l·ªá:   {self.stats['invalid_removed']:>6,}")
        print(f"   ‚îú‚îÄ X√≥a qu√° ng·∫Øn:       {self.stats['short_removed']:>6,}")
        print(f"   ‚îú‚îÄ X√≥a qu√° d√†i:        {self.stats['long_removed']:>6,}")
        print(f"   ‚îî‚îÄ X√≥a tr√πng l·∫∑p:      {self.stats['duplicate_removed']:>6,}")
        print(f"   {'‚îÄ'*35}")
        print(f"   ‚úÖ C√≤n l·∫°i:            {self.stats['final']:>6,}")
        
        # T·ª∑ l·ªá gi·ªØ l·∫°i
        if self.stats['original'] > 0:
            retention = (self.stats['final'] / self.stats['original']) * 100
            print(f"\nüìä T·ª∑ l·ªá gi·ªØ l·∫°i: {retention:.1f}%")
            
            if retention < 50:
                print(f"   ‚ö†Ô∏è  C·∫¢NH B√ÅO: M·∫•t h∆°n 50% d·ªØ li·ªáu!")
                print(f"       C√¢n nh·∫Øc n·ªõi l·ªèng ti√™u ch√≠ (min_words, max_words)")
            elif retention < 70:
                print(f"   ‚ö†Ô∏è  L∆∞u √Ω: M·∫•t {100-retention:.1f}% d·ªØ li·ªáu")
            else:
                print(f"   ‚úÖ T·ªët! Gi·ªØ ƒë∆∞·ª£c ph·∫ßn l·ªõn d·ªØ li·ªáu")
        
        # Chi ti·∫øt c√°c m·∫´u b·ªã x√≥a
        print(f"\nüìù Chi ti·∫øt c√°c m·∫´u b·ªã x√≥a:")
        
        if self.removed_samples['short']:
            print(f"\n   üîç Top 3 c√¢u qu√° ng·∫Øn:")
            for line, reason in self.removed_samples['short'][:3]:
                print(f"      ‚Ä¢ '{line}' - {reason}")
        
        if self.removed_samples['invalid']:
            print(f"\n   üîç Top 3 c√¢u kh√¥ng h·ª£p l·ªá:")
            for line, reason in self.removed_samples['invalid'][:3]:
                preview = line[:50] + '...' if len(line) > 50 else line
                print(f"      ‚Ä¢ '{preview}' - {reason}")
    
    def print_samples(self, data, n=10):
        """Hi·ªÉn th·ªã m·∫´u d·ªØ li·ªáu sau khi l√†m s·∫°ch"""
        print(f"\n{'='*70}")
        print(f"üìù M·∫™U D·ªÆ LI·ªÜU SAU KHI L√ÄM S·∫†CH ({n} c√¢u ƒë·∫ßu)")
        print("="*70)
        
        for i, line in enumerate(data[:n], 1):
            word_count = len(line.split())
            print(f"   {i:2d}. {line} ({word_count} t·ª´)")


# ========== MAIN ==========
if __name__ == "__main__":
    # ƒê∆∞·ªùng d·∫´n
    BASE_DIR = Path(__file__).parent.parent
    INPUT_FILE = BASE_DIR / "data" / "raw" / "dataset.txt"
    OUTPUT_FILE = BASE_DIR / "data" / "processed" / "cleaned_dataset.txt"
    
    # Tham s·ªë l√†m s·∫°ch
    MIN_WORDS = 4    # T·ªëi thi·ªÉu 4 t·ª´
    MAX_WORDS = 30   # T·ªëi ƒëa 30 t·ª´
    
    print("\nüöÄ B·∫ÆT ƒê·∫¶U L√ÄM S·∫†CH DATASET")
    print(f"üì• Input:  {INPUT_FILE}")
    print(f"üì§ Output: {OUTPUT_FILE}")
    print(f"‚öôÔ∏è  C·∫•u h√¨nh: {MIN_WORDS}-{MAX_WORDS} t·ª´")
    
    # Ch·∫°y cleaner
    cleaner = DatasetCleaner()
    success = cleaner.clean(
        input_file=INPUT_FILE,
        output_file=OUTPUT_FILE,
        min_words=MIN_WORDS,
        max_words=MAX_WORDS
    )
    
    if success:
        print(f"\n{'='*70}")
        print("‚úÖ HO√ÄN TH√ÄNH!")
        print("="*70)
        print(f"\nüìå B∆∞·ªõc ti·∫øp theo: Ch·∫°y 3_create_splits.py ƒë·ªÉ t·∫°o train/val/test\n")
    else:
        print(f"\n‚ùå L√†m s·∫°ch th·∫•t b·∫°i!")
        sys.exit(1)