"""
SCRIPT 3: T·∫†O TRAIN/VAL/TEST SPLIT
File: backend/scripts/3_create_splits.py

Ch·∫°y:
  cd backend/scripts
  python 3_create_splits.py
"""

import json
import random
import sys
from pathlib import Path
from collections import Counter

sys.path.append(str(Path(__file__).parent.parent))


class DatasetSplitter:
    """T·∫°o dataset train/val/test v·ªõi partial inputs"""
    
    def __init__(self, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
        assert abs((train_ratio + val_ratio + test_ratio) - 1.0) < 0.01
        self.train_ratio = train_ratio
        self.val_ratio = val_ratio
        self.test_ratio = test_ratio
    
    def create_partial_variants(self, full_text, num_variants=2):
        """
        T·∫°o nhi·ªÅu variants c·ªßa partial input t·ª´ 1 c√¢u
        
        VD: "ƒÉn qu·∫£ nh·ªõ k·∫ª tr·ªìng c√¢y" (5 t·ª´)
        ‚Üí Variants:
          1. input: "ƒÉn qu·∫£" (40%)           target: "nh·ªõ k·∫ª tr·ªìng c√¢y"
          2. input: "ƒÉn qu·∫£ nh·ªõ" (60%)       target: "k·∫ª tr·ªìng c√¢y"
        
        Args:
            full_text: C√¢u ƒë·∫ßy ƒë·ªß
            num_variants: S·ªë variants (2-3 l√† h·ª£p l√Ω)
        """
        words = full_text.split()
        total_words = len(words)
        
        # N·∫øu c√¢u qu√° ng·∫Øn, ch·ªâ t·∫°o 1 variant
        if total_words < 4:
            split_point = max(1, total_words // 2)
            return [{
                'full': full_text,
                'input': ' '.join(words[:split_point]),
                'target': ' '.join(words[split_point:]),
                'split_ratio': split_point / total_words
            }]
        
        variants = []
        
        # T·∫°o c√°c ƒëi·ªÉm c·∫Øt kh√°c nhau
        # VD: C√¢u 8 t·ª´ v·ªõi 2 variants:
        #   - Variant 1: c·∫Øt ·ªü 40% (3 t·ª´ input)
        #   - Variant 2: c·∫Øt ·ªü 60% (5 t·ª´ input)
        
        for i in range(num_variants):
            # T√≠nh ratio: 0.3, 0.4, 0.5, 0.6...
            ratio = 0.3 + (i * 0.15)  # 30%, 45%, 60%...
            
            split_point = max(2, int(total_words * ratio))
            split_point = min(split_point, total_words - 2)  # √çt nh·∫•t 2 t·ª´ c√≤n l·∫°i
            
            variant = {
                'full': full_text,
                'input': ' '.join(words[:split_point]),
                'target': ' '.join(words[split_point:]),
                'split_ratio': round(ratio, 2),
                'input_words': split_point,
                'target_words': total_words - split_point
            }
            
            variants.append(variant)
        
        return variants
    
    def split_data(self, data):
        """Chia dataset theo t·ª∑ l·ªá train/val/test"""
        random.shuffle(data)
        
        n = len(data)
        train_end = int(n * self.train_ratio)
        val_end = train_end + int(n * self.val_ratio)
        
        return {
            'train': data[:train_end],
            'val': data[train_end:val_end],
            'test': data[val_end:]
        }
    
    def create(self, input_file, output_dir, variants_per_proverb=2):
        """
        Pipeline ch√≠nh
        
        Args:
            input_file: File ƒë√£ l√†m s·∫°ch (cadao_cleaned.txt)
            output_dir: Th∆∞ m·ª•c output (data/processed/)
            variants_per_proverb: S·ªë variants m·ªói c√¢u (2-3)
        """
        print("\n" + "="*70)
        print("üì¶ T·∫†O DATASET TRAIN/VAL/TEST")
        print("="*70)
        
        # ƒê·ªçc file
        print(f"\nüìÅ ƒê·ªçc file: {input_file}")
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                proverbs = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {input_file}")
            print(f"   H√£y ch·∫°y 2_clean_data.py tr∆∞·ªõc!")
            return False
        
        print(f"‚úì ƒê·ªçc th√†nh c√¥ng {len(proverbs):,} c√¢u ca dao")
        
        # T·∫°o variants
        print(f"\n{'‚îÄ'*70}")
        print(f"B∆Ø·ªöC 1: T·∫°o {variants_per_proverb} variants cho m·ªói c√¢u")
        
        all_samples = []
        for proverb in proverbs:
            variants = self.create_partial_variants(proverb, num_variants=variants_per_proverb)
            all_samples.extend(variants)
        
        print(f"‚úì T·∫°o ƒë∆∞·ª£c {len(all_samples):,} samples")
        print(f"  ({len(proverbs):,} c√¢u √ó {variants_per_proverb} variants)")
        
        # Ph√¢n t√≠ch ƒë·ªô d√†i input
        input_lengths = [s['input_words'] for s in all_samples]
        target_lengths = [s['target_words'] for s in all_samples]
        
        print(f"\nüìä Th·ªëng k√™ variants:")
        print(f"   ƒê·ªô d√†i input:")
        print(f"      Trung b√¨nh: {sum(input_lengths)/len(input_lengths):.1f} t·ª´")
        print(f"      Min-Max:    {min(input_lengths)}-{max(input_lengths)} t·ª´")
        print(f"   ƒê·ªô d√†i target:")
        print(f"      Trung b√¨nh: {sum(target_lengths)/len(target_lengths):.1f} t·ª´")
        print(f"      Min-Max:    {min(target_lengths)}-{max(target_lengths)} t·ª´")
        
        # Chia dataset
        print(f"\n{'‚îÄ'*70}")
        print("B∆Ø·ªöC 2: Chia train/val/test")
        print(f"   Train: {self.train_ratio*100:.0f}%")
        print(f"   Val:   {self.val_ratio*100:.0f}%")
        print(f"   Test:  {self.test_ratio*100:.0f}%")
        
        splits = self.split_data(all_samples)
        
        # T·∫°o th∆∞ m·ª•c output
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # L∆∞u files
        print(f"\n{'‚îÄ'*70}")
        print("B∆Ø·ªöC 3: L∆∞u files JSON")
        
        for split_name, split_data in splits.items():
            file_path = output_path / f"{split_name}.json"
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(split_data, f, ensure_ascii=False, indent=2)
            
            print(f"   ‚úì {split_name}.json: {len(split_data):>5,} samples")
        
        # Ki·ªÉm tra ch·∫•t l∆∞·ª£ng split
        self.validate_splits(splits)
        
        # Hi·ªÉn th·ªã v√≠ d·ª•
        self.print_examples(splits['train'])
        
        # C·∫£nh b√°o n·∫øu dataset qu√° nh·ªè
        if len(splits['train']) < 200:
            print(f"\n‚ö†Ô∏è  C·∫¢NH B√ÅO: Train set ch·ªâ c√≥ {len(splits['train'])} samples")
            print(f"   M√¥ h√¨nh c√≥ th·ªÉ kh√¥ng h·ªçc t·ªët. Khuy·∫øn ngh·ªã:")
            print(f"   ‚Ä¢ Thu th·∫≠p th√™m ca dao (t·ªëi thi·ªÉu 300+ c√¢u)")
            print(f"   ‚Ä¢ Ho·∫∑c tƒÉng variants_per_proverb l√™n 3-4")
        
        print(f"\n{'='*70}")
        print("‚úÖ HO√ÄN TH√ÄNH!")
        print("="*70)
        print(f"\nüìå B∆∞·ªõc ti·∫øp theo: Train models v·ªõi data ƒë√£ chu·∫©n b·ªã\n")
        
        return True
    
    def validate_splits(self, splits):
        """Ki·ªÉm tra ch·∫•t l∆∞·ª£ng split"""
        print(f"\n{'‚îÄ'*70}")
        print("B∆Ø·ªöC 4: Ki·ªÉm tra ch·∫•t l∆∞·ª£ng")
        
        # Ki·ªÉm tra kh√¥ng c√≥ c√¢u tr√πng gi·ªØa train/val/test
        train_fulls = set(s['full'] for s in splits['train'])
        val_fulls = set(s['full'] for s in splits['val'])
        test_fulls = set(s['full'] for s in splits['test'])
        
        overlap_train_val = train_fulls & val_fulls
        overlap_train_test = train_fulls & test_fulls
        overlap_val_test = val_fulls & test_fulls
        
        total_overlap = len(overlap_train_val) + len(overlap_train_test) + len(overlap_val_test)
        
        if total_overlap == 0:
            print(f"   ‚úÖ Kh√¥ng c√≥ c√¢u tr√πng gi·ªØa train/val/test")
        else:
            print(f"   ‚ö†Ô∏è  Ph√°t hi·ªán {total_overlap} c√¢u tr√πng!")
            if overlap_train_val:
                print(f"      ‚Ä¢ Train-Val: {len(overlap_train_val)} c√¢u")
            if overlap_train_test:
                print(f"      ‚Ä¢ Train-Test: {len(overlap_train_test)} c√¢u")
            if overlap_val_test:
                print(f"      ‚Ä¢ Val-Test: {len(overlap_val_test)} c√¢u")
        
        # Ki·ªÉm tra input kh√¥ng r·ªóng
        issues = 0
        for split_name, split_data in splits.items():
            for i, sample in enumerate(split_data):
                if not sample['input'].strip() or not sample['target'].strip():
                    print(f"   ‚ö†Ô∏è  {split_name}[{i}]: Input ho·∫∑c target r·ªóng!")
                    issues += 1
        
        if issues == 0:
            print(f"   ‚úÖ T·∫•t c·∫£ samples ƒë·ªÅu h·ª£p l·ªá")
        else:
            print(f"   ‚ö†Ô∏è  T√¨m th·∫•y {issues} samples c√≥ v·∫•n ƒë·ªÅ")
    
    def print_examples(self, train_data, n=5):
        """Hi·ªÉn th·ªã v√≠ d·ª•"""
        print(f"\n{'‚îÄ'*70}")
        print(f"üìù V√ç D·ª§ T·ª™ TRAIN SET ({n} samples)")
        print("‚îÄ"*70)
        
        for i, sample in enumerate(train_data[:n], 1):
            print(f"\n   {i}. Full:   {sample['full']}")
            print(f"      Input:  {sample['input']}")
            print(f"      Target: {sample['target']}")
            print(f"      Split:  {sample['split_ratio']*100:.0f}% " +
                  f"({sample['input_words']} t·ª´ input ‚Üí {sample['target_words']} t·ª´ target)")


# ========== MAIN ==========
if __name__ == "__main__":
    # ƒê∆∞·ªùng d·∫´n
    BASE_DIR = Path(__file__).parent.parent
    INPUT_FILE = BASE_DIR / "data" / "processed" / "cleaned_dataset.txt"
    OUTPUT_DIR = BASE_DIR / "data" / "processed"
    
    # C·∫•u h√¨nh
    TRAIN_RATIO = 0.7
    VAL_RATIO = 0.15
    TEST_RATIO = 0.15
    VARIANTS_PER_PROVERB = 2  # TƒÉng l√™n 3 n·∫øu dataset < 400 c√¢u
    
    print("\nüöÄ B·∫ÆT ƒê·∫¶U T·∫†O DATASET SPLITS")
    print(f"üì• Input:  {INPUT_FILE}")
    print(f"üì§ Output: {OUTPUT_DIR}/")
    print(f"‚öôÔ∏è  C·∫•u h√¨nh:")
    print(f"   ‚Ä¢ Train/Val/Test: {TRAIN_RATIO}/{VAL_RATIO}/{TEST_RATIO}")
    print(f"   ‚Ä¢ Variants per proverb: {VARIANTS_PER_PROVERB}")
    
    # Ch·∫°y splitter
    splitter = DatasetSplitter(
        train_ratio=TRAIN_RATIO,
        val_ratio=VAL_RATIO,
        test_ratio=TEST_RATIO
    )
    
    success = splitter.create(
        input_file=INPUT_FILE,
        output_dir=OUTPUT_DIR,
        variants_per_proverb=VARIANTS_PER_PROVERB
    )
    
    if not success:
        sys.exit(1)