from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import json
import pickle
from pathlib import Path


class RetrievalModel:
    """
    Retrieval-based model s·ª≠ d·ª•ng TF-IDF + Cosine Similarity
    
    C√°ch ho·∫°t ƒë·ªông:
    1. Vectorize t·∫•t c·∫£ c√¢u trong dataset th√†nh TF-IDF vectors
    2. Khi c√≥ input, vectorize input
    3. T√≠nh cosine similarity gi·ªØa input v√† t·∫•t c·∫£ c√¢u
    4. Tr·∫£ v·ªÅ top-k c√¢u c√≥ similarity cao nh·∫•t
    
    ∆Øu ƒëi·ªÉm:
    - Lu√¥n tr·∫£ v·ªÅ c√¢u ho√†n ch·ªânh (kh√¥ng generate)
    - X·ª≠ l√Ω t·ªët input m∆° h·ªì
    - Nhanh (vectorize 1 l·∫ßn, inference nhanh)
    
    Nh∆∞·ª£c ƒëi·ªÉm:
    - Kh√¥ng t·∫°o c√¢u m·ªõi (ch·ªâ retrieve)
    - Ph·ª• thu·ªôc v√†o dataset c√≥ ƒë·ªß ƒëa d·∫°ng
    """
    
    def __init__(self, ngram_range=(1, 3), max_features=5000):
        """
        Args:
            ngram_range: (min, max) n-grams ƒë·ªÉ extract
                        (1,3) = unigrams + bigrams + trigrams
            max_features: S·ªë features t·ªëi ƒëa cho TF-IDF
        """
        self.vectorizer = TfidfVectorizer(
            ngram_range=ngram_range,
            max_features=max_features,
            lowercase=True,
            strip_accents=None,  # Gi·ªØ nguy√™n d·∫•u ti·∫øng Vi·ªát
            token_pattern=r'\b\w+\b'
        )
        
        self.database = []  # List of full sentences
        self.vectors = None  # TF-IDF matrix
        self.is_trained = False
    
    def train(self, train_data):
        """
        Hu·∫•n luy·ªán model = X√¢y d·ª±ng database + vectorize
        
        Args:
            train_data: List of dicts [{'full': '...', ...}]
        """
        print(f"\n{'‚îÄ'*60}")
        print(f"üîÑ TRAINING RETRIEVAL MODEL")
        print(f"{'‚îÄ'*60}")
        
        # L·∫•y unique sentences
        seen = set()
        for item in train_data:
            sentence = item['full']
            if sentence not in seen:
                self.database.append(sentence)
                seen.add(sentence)
        
        print(f"üìä Database: {len(self.database)} unique sentences")
        
        # Vectorize t·∫•t c·∫£ c√¢u
        print(f"üîÑ Vectorizing with TF-IDF...")
        self.vectors = self.vectorizer.fit_transform(self.database)
        
        print(f"‚úì Vector shape: {self.vectors.shape}")
        print(f"‚úì Vocabulary size: {len(self.vectorizer.vocabulary_):,}")
        
        # Ph√¢n t√≠ch top features
        feature_names = self.vectorizer.get_feature_names_out()
        print(f"\nüìù Top 10 features (t·ª´ quan tr·ªçng nh·∫•t):")
        
        # T√≠nh IDF scores
        idf_scores = self.vectorizer.idf_
        top_indices = np.argsort(idf_scores)[:10]  # IDF th·∫•p = xu·∫•t hi·ªán nhi·ªÅu
        
        for i, idx in enumerate(top_indices, 1):
            print(f"   {i:2d}. '{feature_names[idx]}' (IDF: {idf_scores[idx]:.2f})")
        
        self.is_trained = True
    
    def retrieve(self, query, top_k=10):
        """
        T√¨m top-k c√¢u gi·ªëng nh·∫•t
        
        Args:
            query: Input string
            top_k: S·ªë c√¢u tr·∫£ v·ªÅ
        
        Returns:
            List of (sentence, similarity_score) tuples
        """
        if not self.is_trained:
            raise ValueError("Model ch∆∞a ƒë∆∞·ª£c train!")
        
        # Vectorize query
        query_vec = self.vectorizer.transform([query.lower()])
        
        # T√≠nh similarity v·ªõi t·∫•t c·∫£ c√¢u
        similarities = cosine_similarity(query_vec, self.vectors)[0]
        
        # L·∫•y top-k indices
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        # Tr·∫£ v·ªÅ (sentence, score)
        results = []
        for idx in top_indices:
            if similarities[idx] > 0:  # Ch·ªâ l·∫•y n·∫øu c√≥ similarity > 0
                results.append((self.database[idx], float(similarities[idx])))
        
        return results
    
    def predict_multiple(self, partial_input, top_k=3, min_similarity=0.05):
        """
        Tr·∫£ v·ªÅ top-k candidates cho API
        
        Args:
            partial_input: Input string
            top_k: S·ªë candidates
            min_similarity: Ng∆∞·ª°ng similarity t·ªëi thi·ªÉu
        
        Returns:
            List of dicts [{'text': '...', 'confidence': 0.9, 'model': 'retrieval'}]
        """
        # Retrieve top candidates
        retrieved = self.retrieve(partial_input, top_k=top_k*2)  # L·∫•y nhi·ªÅu h∆°n ƒë·ªÉ filter
        
        candidates = []
        
        for sentence, similarity in retrieved:
            # Filter theo threshold
            if similarity < min_similarity:
                continue
            
            # Map similarity ‚Üí confidence (0-1)
            # Similarity th∆∞·ªùng trong kho·∫£ng 0.1-0.8
            # Scale l√™n ƒë·ªÉ confidence r√µ r√†ng h∆°n
            confidence = min(0.99, similarity * 1.2)
            
            candidates.append({
                'text': sentence,
                'confidence': round(confidence, 3),
                'model': 'retrieval',
                'similarity': round(similarity, 3)
            })
            
            if len(candidates) >= top_k:
                break
        
        # Fallback n·∫øu kh√¥ng t√¨m th·∫•y g√¨
        if not candidates:
            import random
            random_sentence = random.choice(self.database) if self.database else partial_input
            candidates = [{
                'text': random_sentence,
                'confidence': 0.05,
                'model': 'retrieval',
                'similarity': 0.0,
                'method': 'fallback'
            }]
        
        return candidates
    
    def predict(self, partial_input):
        """
        Tr·∫£ v·ªÅ 1 k·∫øt qu·∫£ t·ªët nh·∫•t (wrapper)
        """
        candidates = self.predict_multiple(partial_input, top_k=1)
        return candidates[0]['text'] if candidates else partial_input
    
    def evaluate(self, test_data):
        """
        ƒê√°nh gi√° model tr√™n test set
        
        Metrics:
        - Exact match accuracy
        - Top-3 accuracy (c√¢u ƒë√∫ng c√≥ trong top 3 kh√¥ng)
        - Average similarity score
        """
        print(f"\n{'‚îÄ'*60}")
        print(f"üìä EVALUATING RETRIEVAL MODEL")
        print(f"{'‚îÄ'*60}")
        
        exact_correct = 0
        top3_correct = 0
        total = len(test_data)
        similarities = []
        
        for item in test_data:
            # Predict
            candidates = self.predict_multiple(item['input'], top_k=3)
            
            # Check exact match
            if candidates[0]['text'] == item['full']:
                exact_correct += 1
            
            # Check top-3
            top3_texts = [c['text'] for c in candidates]
            if item['full'] in top3_texts:
                top3_correct += 1
            
            # Similarity score
            if candidates:
                similarities.append(candidates[0]['similarity'])
        
        exact_acc = exact_correct / total if total > 0 else 0
        top3_acc = top3_correct / total if total > 0 else 0
        avg_sim = sum(similarities) / len(similarities) if similarities else 0
        
        print(f"Test samples: {total}")
        print(f"Exact matches: {exact_correct} ({exact_acc:.1%})")
        print(f"Top-3 matches: {top3_correct} ({top3_acc:.1%})")
        print(f"Avg similarity: {avg_sim:.3f}")
        
        return {
            'exact_accuracy': exact_acc,
            'top3_accuracy': top3_acc,
            'avg_similarity': avg_sim,
            'exact_correct': exact_correct,
            'top3_correct': top3_correct,
            'total': total
        }
    
    def save(self, file_path):
        """L∆∞u model"""
        data = {
            'vectorizer': self.vectorizer,
            'database': self.database,
            'vectors': self.vectors
        }
        
        with open(file_path, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"‚úì Model saved to {file_path}")
    
    def load(self, file_path):
        """Load model"""
        with open(file_path, 'rb') as f:
            data = pickle.load(f)
        
        self.vectorizer = data['vectorizer']
        self.database = data['database']
        self.vectors = data['vectors']
        self.is_trained = True
        
        print(f"‚úì Model loaded from {file_path}")


# ========== SCRIPT TRAINING ==========
def train_retrieval_model():
    """Script ƒë·ªÉ train v√† test model"""
    
    print("\n" + "="*70)
    print("üöÄ RETRIEVAL MODEL TRAINING")
    print("="*70)
    
    # ƒê∆∞·ªùng d·∫´n
    BASE_DIR = Path(__file__).parent.parent
    DATA_DIR = BASE_DIR / "data" / "processed"
    MODEL_DIR = BASE_DIR / "trained_models"
    
    MODEL_DIR.mkdir(exist_ok=True)
    
    # Load data
    print(f"\nüìÇ Loading data...")
    
    with open(DATA_DIR / "train.json", 'r', encoding='utf-8') as f:
        train_data = json.load(f)
    
    with open(DATA_DIR / "test.json", 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    print(f"‚úì Train: {len(train_data)} samples")
    print(f"‚úì Test:  {len(test_data)} samples")
    
    # Train model
    model = RetrievalModel(ngram_range=(1, 3), max_features=5000)
    model.train(train_data)
    
    # Test predictions
    print(f"\n{'‚îÄ'*60}")
    print("üß™ TEST PREDICTIONS")
    print(f"{'‚îÄ'*60}")
    
    test_inputs = [
        "ƒÉn",
        "ƒÉn qu·∫£",
        "ƒÉn qu·∫£ nh·ªõ",
        "c√≥ c√¥ng",
        "c√≥ c√¥ng m√†i s·∫Øt",
        "g·∫ßn m·ª±c",
        "h·ªçc th·∫ßy kh√¥ng"
    ]
    
    for inp in test_inputs:
        print(f"\nüìù Input: '{inp}'")
        candidates = model.predict_multiple(inp, top_k=3)
        
        for i, cand in enumerate(candidates, 1):
            print(f"   {i}. {cand['text']}")
            print(f"      üìä Confidence: {cand['confidence']:.1%} | Similarity: {cand['similarity']:.3f}")
    
    # Evaluate
    metrics = model.evaluate(test_data[:100])
    
    # Save model
    model_path = MODEL_DIR / "retrieval_model.pkl"
    model.save(model_path)
    
    print(f"\n{'='*70}")
    print("‚úÖ TRAINING COMPLETE!")
    print("="*70)
    print(f"\nüìä Summary:")
    print(f"   ‚Ä¢ Database size: {len(model.database):,} sentences")
    print(f"   ‚Ä¢ Vector dimension: {model.vectors.shape[1]:,}")
    print(f"   ‚Ä¢ Exact accuracy: {metrics['exact_accuracy']:.1%}")
    print(f"   ‚Ä¢ Top-3 accuracy: {metrics['top3_accuracy']:.1%}")
    print(f"   ‚Ä¢ Avg similarity: {metrics['avg_similarity']:.3f}")
    print(f"   ‚Ä¢ Model saved: {model_path}")
    print()


# ========== MAIN ==========
if __name__ == "__main__":
    train_retrieval_model()