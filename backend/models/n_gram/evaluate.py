import pandas as pd
import random
import time
import difflib
import os
from pyvi import ViTokenizer
from gen_mul_layer import BidirectionalBeamGenerator
import matplotlib.pyplot as plt
import seaborn as sns

# CÃ i Ä‘áº·t tqdm Ä‘á»ƒ theo dÃµi tiáº¿n Ä‘á»™ (vÃ¬ cháº¡y full data sáº½ lÃ¢u)
try:
    from tqdm import tqdm
except ImportError:
    import subprocess
    import sys
    subprocess.check_call([sys.executable, "-m", "pip", "install", "tqdm"])
    from tqdm import tqdm

# ==============================================================================
# 1. HÃ€M Táº O Dá»® LIá»†U TEST TOÃ€N DIá»†N (FULL STRESS TEST)
# ==============================================================================
def create_full_stress_test(original_file, max_input_ratio=0.7):
    """
    Táº¡o ma tráº­n test phá»§ kÃ­n toÃ n bá»™ dataset.
    KhÃ´ng láº¥y máº«u ngáº«u nhiÃªn mÃ  vÃ©t cáº¡n má»i trÆ°á»ng há»£p.
    """
    print(f"ğŸ›  Äang xÃ¢y dá»±ng ma tráº­n test toÃ n diá»‡n tá»« {original_file}...")
    test_data = []

    # Táº¡o file máº«u náº¿u chÆ°a cÃ³
    if not os.path.exists(original_file):
        sample_text = """
        CÃ´ng cha nhÆ° nÃºi ThÃ¡i SÆ¡n
        NghÄ©a máº¹ nhÆ° nÆ°á»›c trong nguá»“n cháº£y ra.
        Anh em nhÆ° thá»ƒ tay chÃ¢n
        RÃ¡ch lÃ nh Ä‘Ã¹m bá»c dá»Ÿ hay Ä‘á»¡ Ä‘áº§n.
        """
        with open(original_file, "w", encoding="utf-8") as f:
            f.write(sample_text.strip())

    with open(original_file, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f if line.strip()]

    # Duyá»‡t qua tá»«ng dÃ²ng vá»›i thanh tiáº¿n trÃ¬nh
    for line in tqdm(lines, desc="Processing Data"):
        tokenized_line = ViTokenizer.tokenize(line).lower()
        words = tokenized_line.split()
        n = len(words)

        # Bá» qua cÃ¢u quÃ¡ ngáº¯n
        if n < 3: continue

        ground_truth = " ".join(words)
        max_test_len = max(1, int(n * max_input_ratio))

        # -------------------------------------------------
        # 1. CASE START (Äáº§u cÃ¢u -> Test Gen XuÃ´i)
        # -------------------------------------------------
        for k in range(1, max_test_len + 1):
            test_data.append({
                "Loáº¡i": "Start (Äáº§u)",
                "Input Len": k,
                "Input": " ".join(words[:k]),
                "Ground_Truth": ground_truth
            })

        # -------------------------------------------------
        # 2. CASE END (Cuá»‘i cÃ¢u -> Test Gen NgÆ°á»£c)
        # -------------------------------------------------
        for k in range(1, max_test_len + 1):
            test_data.append({
                "Loáº¡i": "End (Cuá»‘i)",
                "Input Len": k,
                "Input": " ".join(words[-k:]),
                "Ground_Truth": ground_truth
            })

        # -------------------------------------------------
        # 3. CASE MID (Giá»¯a cÃ¢u -> Test Gen 2 Chiá»u)
        # -------------------------------------------------
        if n >= 5:
            # QuÃ©t Ä‘á»™ dÃ i input tá»« 1 Ä‘áº¿n max
            for k in range(1, max_test_len + 1):
                # TrÆ°á»£t cá»­a sá»•
                for start_idx in range(1, n - k):
                    test_data.append({
                        "Loáº¡i": "Mid (Giá»¯a)",
                        "Input Len": k,
                        "Input": " ".join(words[start_idx : start_idx + k]),
                        "Ground_Truth": ground_truth
                    })

    df = pd.DataFrame(test_data)
    print(f"âœ… ÄÃ£ táº¡o {len(df)} máº«u test cases tá»« {len(lines)} cÃ¢u gá»‘c.")
    return df

# ==============================================================================
# 2. HÃ€M ÄÃNH GIÃ CHI TIáº¾T
# ==============================================================================
def evaluate_full_dataset(generator, test_df):
    print(f"\nğŸš€ Báº®T Äáº¦U ÄÃNH GIÃ TRÃŠN {len(test_df)} MáºªU...")

    results = []
    correct_count = 0
    total_similarity = 0
    start_time = time.time()

    # Cháº¡y vÃ²ng láº·p Ä‘Ã¡nh giÃ¡
    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc="Evaluating"):
        inp = row['Input']
        truth = row['Ground_Truth']

        try:
            # --- Gá»ŒI MODEL BIDIRECTIONAL BEAM ---
            # HÃ m cá»§a báº¡n lÃ  generate_best_cases
            gen_output = generator.generate_best_cases(inp, num_results=1)

            if not gen_output:
                pred_clean = ""
                score = -999.0
            else:
                # Unpack káº¿t quáº£: (score, text)
                score, formatted_text = gen_output[0]

                # --- CHUáº¨N HÃ“A Káº¾T QUáº¢ Äá»‚ SO SÃNH ---
                # Model tráº£ vá» dáº¡ng thÆ¡ Ä‘áº¹p -> Cáº§n Ä‘Æ°a vá» dáº¡ng tokenized thÆ°á»ng
                pred_clean = formatted_text.replace("\n", " ").lower()
                pred_clean = pred_clean.replace(".", "").replace(",", "").strip()
                # Quan trá»ng: Pháº£i tokenize láº¡i thÃ¬ má»›i khá»›p vá»›i Ground Truth (vd: thÃ¡i_sÆ¡n)
                pred_clean = ViTokenizer.tokenize(pred_clean)

        except Exception as e:
            pred_clean = f"Error: {e}"
            score = -999.0

        # Chuáº©n hÃ³a Ground Truth
        truth_clean = truth.replace(".", "").strip()

        # --- CHáº¤M ÄIá»‚M ---
        # 1. Exact Match
        is_exact = 1 if pred_clean == truth_clean else 0
        correct_count += is_exact

        # 2. Similarity
        sim = difflib.SequenceMatcher(None, pred_clean, truth_clean).ratio()
        total_similarity += sim

        results.append({
            "Loáº¡i": row['Loáº¡i'],
            "Input Len": row['Input Len'],
            "Input": inp,
            "Káº¿t quáº£ Gen": pred_clean,
            "ÄÃ¡p Ã¡n Gá»‘c": truth_clean,
            "Äiá»ƒm Model": score,
            "ÄÃºng": is_exact,
            "Äá»™ giá»‘ng": sim
        })

    total_time = time.time() - start_time

    # --- Táº O BÃO CÃO ---
    res_df = pd.DataFrame(results)
    if len(test_df) > 0:
        acc = correct_count / len(test_df) * 100
        avg_sim = total_similarity / len(test_df) * 100
    else:
        acc = 0; avg_sim = 0

    print("\n" + "="*60)
    print(f"ğŸ“Š BÃO CÃO HIá»†U NÄ‚NG TOÃ€N DIá»†N")
    print("="*60)
    print(f"â±  Thá»i gian: {total_time:.2f}s ({total_time/len(test_df)*1000:.1f} ms/cÃ¢u)")
    print(f"ğŸ¯ Äá»™ chÃ­nh xÃ¡c tuyá»‡t Ä‘á»‘i (Exact Match): {acc:.2f}%")
    print(f"â‰ˆ  Äá»™ tÆ°Æ¡ng Ä‘á»“ng trung bÃ¬nh (Similarity):  {avg_sim:.2f}%")



    # --- PHÃ‚N TÃCH 1: THEO Vá»Š TRÃ ---
    print("-" * 60)
    print("1. PHÃ‚N TÃCH THEO Vá»Š TRÃ (Start/Mid/End):")
    # Xem model giá»i chiá»u nÃ o hÆ¡n
    group_type = res_df.groupby("Loáº¡i")[["ÄÃºng", "Äá»™ giá»‘ng"]].mean() * 100
    print(group_type.round(2))

    # --- PHÃ‚N TÃCH 2: THEO Äá»˜ DÃ€I INPUT ---
    print("-" * 60)
    print("2. PHÃ‚N TÃCH THEO Äá»˜ DÃ€I INPUT (10 Ä‘á»™ dÃ i Ä‘áº§u):")
    # Input cÃ ng ngáº¯n cÃ ng khÃ³ Ä‘oÃ¡n
    group_len = res_df.groupby("Input Len")[["ÄÃºng", "Äá»™ giá»‘ng"]].mean().head(10) * 100
    print(group_len.round(2))
    print("="*60)

    return res_df

# ==============================================================================
# 3. HÃ€M Váº¼ Äá»’ THá»Š
# ==============================================================================
def plot_evaluation_results(res_df, output_dir):
    """
    Váº½ 4 Ä‘á»“ thá»‹ phÃ¢n tÃ­ch chi tiáº¿t káº¿t quáº£ Ä‘Ã¡nh giÃ¡
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
    
    # Cáº¥u hÃ¬nh seaborn
    sns.set_style("whitegrid")
    plt.rcParams['figure.figsize'] = (14, 10)
    plt.rcParams['font.size'] = 10
    
    # --- Äá»’ THá»Š 1: Hiá»‡u nÄƒng theo vá»‹ trÃ­ (Start/Mid/End) ---
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # Plot 1: Bar chart - Accuracy by Type
    type_stats = res_df.groupby("Loáº¡i")[["ÄÃºng", "Äá»™ giá»‘ng"]].mean() * 100
    type_stats.plot(kind='bar', ax=axes[0, 0], color=['#2ecc71', '#3498db'])
    axes[0, 0].set_title('Hiá»‡u nÄƒng theo vá»‹ trÃ­ (Start/Mid/End)', fontsize=12, fontweight='bold')
    axes[0, 0].set_ylabel('Pháº§n trÄƒm (%)')
    axes[0, 0].set_xlabel('Vá»‹ trÃ­')
    axes[0, 0].legend(['ChÃ­nh xÃ¡c', 'Äá»™ giá»‘ng'])
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    # Plot 2: Line chart - Performance by Input Length
    len_stats = res_df.groupby("Input Len")[["ÄÃºng", "Äá»™ giá»‘ng"]].mean() * 100
    axes[0, 1].plot(len_stats.index, len_stats["ÄÃºng"], marker='o', label='ChÃ­nh xÃ¡c', color='#2ecc71', linewidth=2)
    axes[0, 1].plot(len_stats.index, len_stats["Äá»™ giá»‘ng"], marker='s', label='Äá»™ giá»‘ng', color='#3498db', linewidth=2)
    axes[0, 1].set_title('Hiá»‡u nÄƒng theo Ä‘á»™ dÃ i input', fontsize=12, fontweight='bold')
    axes[0, 1].set_xlabel('Äá»™ dÃ i Input (tá»«)')
    axes[0, 1].set_ylabel('Pháº§n trÄƒm (%)')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Plot 3: Pie chart - Distribution of Correct/Wrong
    correct_dist = res_df["ÄÃºng"].value_counts()
    labels = ['Sai', 'ÄÃºng'] if len(correct_dist) > 1 else ['ÄÃºng']
    colors = ['#e74c3c', '#2ecc71']
    axes[1, 0].pie(correct_dist.values, labels=[f'{l}\n{v} ({v/len(res_df)*100:.1f}%)' for l, v in zip(labels, correct_dist.values)], 
                   autopct='', colors=colors[:len(correct_dist)], startangle=90)
    axes[1, 0].set_title('PhÃ¢n phá»‘i ÄÃºng/Sai', fontsize=12, fontweight='bold')
    
    # Plot 4: Scatter - Input Length vs Similarity
    scatter = axes[1, 1].scatter(res_df["Input Len"], res_df["Äá»™ giá»‘ng"] * 100, 
                                 c=res_df["ÄÃºng"], cmap='RdYlGn', s=30, alpha=0.6, edgecolors='black', linewidth=0.5)
    axes[1, 1].set_title('Input Length vs Äá»™ Giá»‘ng', fontsize=12, fontweight='bold')
    axes[1, 1].set_xlabel('Äá»™ dÃ i Input (tá»«)')
    axes[1, 1].set_ylabel('Äá»™ giá»‘ng (%)')
    cbar = plt.colorbar(scatter, ax=axes[1, 1])
    cbar.set_label('ÄÃºng/Sai')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # LÆ°u file
    output_file = os.path.join(output_dir, 'evaluation_report.png')
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"[SAVED] Äá»“ thá»‹ lÆ°u táº¡i: {output_file}")
    plt.close()
    
    # --- Äá»’ THá»Š Bá»” SUNG: Chi tiáº¿t theo tá»«ng type ---
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    for idx, type_name in enumerate(res_df["Loáº¡i"].unique()):
        type_data = res_df[res_df["Loáº¡i"] == type_name]
        len_stats_type = type_data.groupby("Input Len")[["Äá»™ giá»‘ng"]].mean() * 100
        
        axes[idx].plot(len_stats_type.index, len_stats_type["Äá»™ giá»‘ng"], 
                      marker='o', linewidth=2, markersize=6, color='#3498db')
        axes[idx].set_title(f'{type_name}', fontsize=11, fontweight='bold')
        axes[idx].set_xlabel('Äá»™ dÃ i Input (tá»«)')
        axes[idx].set_ylabel('Äá»™ giá»‘ng (%)')
        axes[idx].grid(True, alpha=0.3)
        axes[idx].set_ylim(0, 105)
    
    plt.tight_layout()
    output_file = os.path.join(output_dir, 'evaluation_by_type.png')
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"[SAVED] Äá»“ thá»‹ chi tiáº¿t lÆ°u táº¡i: {output_file}")
    plt.close()
    
    print(f"âœ… Táº¥t cáº£ Ä‘á»“ thá»‹ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o: {output_dir}")
    return res_df

# ==============================================================================
# 4. CHáº Y THá»°C Táº¾
# ==============================================================================
if __name__ == "__main__":
    # Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    data_dir = os.path.join(project_root, "data")
    ngram_dir = os.path.dirname(os.path.abspath(__file__))
    checkpoint_dir = os.path.join(ngram_dir, "checkpoint")
    eval_dir = os.path.join(project_root, "evaluation", "n_gram")
    
    model_file = os.path.join(checkpoint_dir, 'model.bin')
    data_file = os.path.join(data_dir, 'train_data_seg.txt')
    raw_dataset = os.path.join(data_dir, "dataset.txt")

    try:
        # Kiá»ƒm tra biáº¿n beam_gen
        if 'beam_gen' not in globals():
            print("âš ï¸ Äang khá»Ÿi táº¡o láº¡i Beam Generator...")
            if os.path.exists(model_file):
                beam_gen = BidirectionalBeamGenerator(model_file, data_file, n_gram_order=5)
            else:
                print("âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y model.bin.")
                beam_gen = None

        if beam_gen:
            # 1. Táº¡o dataset test toÃ n diá»‡n
            full_test_df = create_full_stress_test(raw_dataset, max_input_ratio=0.7)
            
            # LÆ°u file test data
            test_data_file = os.path.join(data_dir, 'dataset_ngram_test.csv')
            full_test_df.to_csv(test_data_file, index=False, encoding='utf-8')
            print(f"âœ… Test data Ä‘Ã£ lÆ°u táº¡i: {test_data_file}")

            # 2. Cháº¡y Ä‘Ã¡nh giÃ¡
            df_final = evaluate_full_dataset(beam_gen, full_test_df)

            # 3. Váº½ Ä‘á»“ thá»‹ vÃ  lÆ°u
            plot_evaluation_results(df_final, eval_dir)

            # 4. Xuáº¥t cÃ¡c cÃ¢u sai Ä‘á»ƒ debug
            print("\nğŸ” TOP 5 CÃ‚U SAI ÄIá»‚N HÃŒNH:")
            wrong_cases = df_final[df_final["ÄÃºng"] == 0].head(5)
            if not wrong_cases.empty:
                pd.set_option('display.max_colwidth', None)
                print(wrong_cases[['Loáº¡i', 'Input', 'Káº¿t quáº£ Gen', 'ÄÃ¡p Ã¡n Gá»‘c', 'Äá»™ giá»‘ng']])
            else:
                print("âœ… Xuáº¥t sáº¯c! Model Ä‘Ãºng 100%.")

    except Exception as e:
        print(f"Lá»—i: {e}")