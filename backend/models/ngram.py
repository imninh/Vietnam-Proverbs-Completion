from collections import defaultdict, Counter
import json
import pickle
from pathlib import Path


class NgramModel:
    """
    M√¥ h√¨nh N-gram ƒë·ªÉ d·ª± ƒëo√°n t·ª´ ti·∫øp theo
    
    C√°ch ho·∫°t ƒë·ªông:
    1. Training: ƒê·∫øm t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa c√°c n-grams
    2. Prediction: T√¨m t·ª´ c√≥ x√°c su·∫•t cao nh·∫•t sau context
    
    VD v·ªõi trigram (n=3):
    - Context: ("ƒÉn", "qu·∫£")
    - T·ª´ ti·∫øp theo c√≥ th·ªÉ l√†: "nh·ªõ" (80%), "ng·ªçt" (20%)
    """
    
    def __init__(self, n=3):
        """
        Args:
            n: ƒê·ªô d√†i context (3 = trigram l√† t·ªët nh·∫•t cho ti·∫øng Vi·ªát)
        """
        self.n = n
        
        # Dictionary l∆∞u n-grams
        # Key: tuple c·ªßa n-1 t·ª´ (context)
        # Value: Counter c·ªßa t·ª´ ti·∫øp theo v√† t·∫ßn su·∫•t
        # VD: {('ƒÉn', 'qu·∫£'): Counter({'nh·ªõ': 10, 'ng·ªçt': 2})}
        self.ngrams = defaultdict(Counter)
        
        # L∆∞u to√†n b·ªô c√¢u ƒë·ªÉ fallback khi kh√¥ng t√¨m th·∫•y
        self.full_sentences = []
        
        # Th·ªëng k√™
        self.vocab_size = 0
        self.total_ngrams = 0
    
    def train(self, train_data):
        """
        Hu·∫•n luy·ªán m√¥ h√¨nh t·ª´ dataset
        
        Args:
            train_data: List of dicts [{'full': '...', 'input': '...', 'target': '...'}]
        """
        print(f"\n{'‚îÄ'*60}")
        print(f"üîÑ TRAINING N-GRAM MODEL (n={self.n})")
        print(f"{'‚îÄ'*60}")
        
        # L∆∞u t·∫•t c·∫£ c√¢u ƒë·∫ßy ƒë·ªß
        seen_sentences = set()
        for item in train_data:
            sentence = item['full']
            if sentence not in seen_sentences:
                self.full_sentences.append(sentence)
                seen_sentences.add(sentence)
        
        print(f"üìä Dataset: {len(train_data)} samples, {len(self.full_sentences)} unique sentences")
        
        # ƒê·∫øm n-grams
        vocabulary = set()
        
        for item in train_data:
            words = item['full'].split()
            vocabulary.update(words)
            
            # T·∫°o n-grams t·ª´ c√¢u
            # VD: "ƒÉn qu·∫£ nh·ªõ k·∫ª" v·ªõi n=3
            # ‚Üí contexts: [("ƒÉn", "qu·∫£"), ("qu·∫£", "nh·ªõ")]
            # ‚Üí next_words: ["nh·ªõ", "k·∫ª"]
            
            for i in range(len(words) - self.n):
                # L·∫•y n-1 t·ª´ l√†m context
                context = tuple(words[i:i+self.n-1])
                
                # T·ª´ ti·∫øp theo
                next_word = words[i+self.n-1]
                
                # ƒê·∫øm
                self.ngrams[context][next_word] += 1
                self.total_ngrams += 1
        
        self.vocab_size = len(vocabulary)
        
        print(f"‚úì Vocabulary size: {self.vocab_size:,} t·ª´")
        print(f"‚úì Total n-grams: {self.total_ngrams:,}")
        print(f"‚úì Unique contexts: {len(self.ngrams):,}")
        
        # Th·ªëng k√™ ph√¢n b·ªë
        context_sizes = [sum(counter.values()) for counter in self.ngrams.values()]
        avg_size = sum(context_sizes) / len(context_sizes) if context_sizes else 0
        
        print(f"‚úì Avg words per context: {avg_size:.1f}")
        
        # V√≠ d·ª• n-grams
        print(f"\nüìù V√≠ d·ª• n-grams h·ªçc ƒë∆∞·ª£c:")
        for i, (context, counter) in enumerate(list(self.ngrams.items())[:3]):
            context_str = ' '.join(context)
            top_3 = counter.most_common(3)
            print(f"   {i+1}. '{context_str}' ‚Üí")
            for word, count in top_3:
                prob = count / sum(counter.values())
                print(f"      ‚Ä¢ '{word}' ({prob:.1%}, {count} l·∫ßn)")
    
    def predict_next_word(self, context_words):
        """
        D·ª± ƒëo√°n 1 t·ª´ ti·∫øp theo
        
        Args:
            context_words: List of words (VD: ["ƒÉn", "qu·∫£"])
        
        Returns:
            (word, confidence) ho·∫∑c (None, 0) n·∫øu kh√¥ng t√¨m th·∫•y
        """
        # L·∫•y n-1 t·ª´ cu·ªëi l√†m context
        context = tuple(context_words[-(self.n-1):])
        
        if context not in self.ngrams:
            return None, 0.0
        
        # T√¨m t·ª´ xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
        counter = self.ngrams[context]
        most_common_word, count = counter.most_common(1)[0]
        
        # T√≠nh confidence (x√°c su·∫•t)
        total_count = sum(counter.values())
        confidence = count / total_count
        
        return most_common_word, confidence
    
    def predict(self, partial_input, max_words=15):
        """
        D·ª± ƒëo√°n ho√†n thi·ªán c√¢u (generate t·ª´ng t·ª´)
        
        Args:
            partial_input: Chu·ªói input (VD: "ƒÉn qu·∫£ nh·ªõ")
            max_words: S·ªë t·ª´ t·ªëi ƒëa ƒë·ªÉ generate
        
        Returns:
            C√¢u ho√†n ch·ªânh
        """
        words = partial_input.strip().split()
        result = words.copy()
        
        # Generate t·ª´ng t·ª´
        for _ in range(max_words):
            next_word, confidence = self.predict_next_word(result)
            
            if next_word is None:
                # Kh√¥ng t√¨m th·∫•y ‚Üí d·ª´ng
                break
            
            result.append(next_word)
            
            # D·ª´ng n·∫øu c√¢u ƒë√£ ƒë·ªß d√†i (heuristic)
            if len(result) >= len(words) + 8:
                break
            
            # D·ª´ng n·∫øu confidence qu√° th·∫•p (t·ª´ hi·∫øm)
            if confidence < 0.1:
                break
        
        return ' '.join(result)
    
    def predict_multiple(self, partial_input, top_k=3):
        """
        Tr·∫£ v·ªÅ nhi·ªÅu candidates (d√πng cho API) - IMPROVED VERSION
        
        Args:
            partial_input: Chu·ªói input
            top_k: S·ªë candidates tr·∫£ v·ªÅ
        
        Returns:
            List of dicts [{'text': '...', 'confidence': 0.9, 'model': 'ngram'}]
        """
        # Normalize input
        words = partial_input.strip().lower().split()
        input_text = ' '.join(words)
        
        candidates = []
        
        # STRATEGY 1: Exact prefix match
        for sentence in self.full_sentences:
            sentence_lower = sentence.lower()
            
            # Ki·ªÉm tra c√¢u c√≥ b·∫Øt ƒë·∫ßu b·∫±ng input kh√¥ng
            if sentence_lower.startswith(input_text):
                # T√≠nh confidence d·ª±a tr√™n ƒë·ªô overlap
                overlap_ratio = len(input_text) / len(sentence_lower)
                confidence = min(0.95, overlap_ratio + 0.2)  # Boost confidence
                
                candidates.append({
                    'text': sentence,
                    'confidence': round(confidence, 3),
                    'model': 'ngram',
                    'method': 'exact_match'
                })
        
        # STRATEGY 2: Fuzzy match (ch·ª©a c√°c t·ª´ c·ªßa input)
        if len(candidates) < top_k:
            for sentence in self.full_sentences:
                sentence_lower = sentence.lower()
                
                # Ki·ªÉm tra c√°c t·ª´ input c√≥ trong c√¢u kh√¥ng
                words_in_sentence = sum(1 for word in words if word in sentence_lower)
                match_ratio = words_in_sentence / len(words) if words else 0
                
                # Ch·ªâ l·∫•y n·∫øu match >= 50% v√† ch∆∞a c√≥ trong candidates
                if match_ratio >= 0.5 and sentence not in [c['text'] for c in candidates]:
                    confidence = match_ratio * 0.6  # Lower confidence
                    
                    candidates.append({
                        'text': sentence,
                        'confidence': round(confidence, 3),
                        'model': 'ngram',
                        'method': 'fuzzy_match'
                    })
        
        # STRATEGY 3: Generate v·ªõi n-gram
        if len(candidates) < top_k:
            generated = self.predict(partial_input)
            
            # Ch·ªâ th√™m n·∫øu kh√°c v·ªõi input v√† ch∆∞a c√≥
            if generated.lower() != input_text and generated not in [c['text'] for c in candidates]:
                candidates.append({
                    'text': generated,
                    'confidence': 0.4,
                    'model': 'ngram',
                    'method': 'generated'
                })
        
        # Sort theo confidence
        candidates.sort(key=lambda x: x['confidence'], reverse=True)
        
        # L·∫•y top-k
        candidates = candidates[:top_k]
        
        # STRATEGY 4: Fallback n·∫øu v·∫´n kh√¥ng c√≥
        if not candidates:
            import random
            random_sentence = random.choice(self.full_sentences) if self.full_sentences else partial_input
            candidates = [{
                'text': random_sentence,
                'confidence': 0.1,
                'model': 'ngram',
                'method': 'fallback'
            }]
        
        return candidates
    
    def evaluate(self, test_data):
        """
        ƒê√°nh gi√° model tr√™n test set
        
        Args:
            test_data: List of dicts [{'full': '...', 'input': '...', 'target': '...'}]
        
        Returns:
            Dict v·ªõi c√°c metrics
        """
        print(f"\n{'‚îÄ'*60}")
        print(f"üìä EVALUATING N-GRAM MODEL")
        print(f"{'‚îÄ'*60}")
        
        correct = 0
        total = len(test_data)
        
        for item in test_data:
            predicted = self.predict(item['input'])
            
            # Exact match
            if predicted == item['full']:
                correct += 1
        
        accuracy = correct / total if total > 0 else 0
        
        print(f"Test samples: {total}")
        print(f"Exact matches: {correct}")
        print(f"Accuracy: {accuracy:.2%}")
        
        return {
            'accuracy': accuracy,
            'correct': correct,
            'total': total
        }
    
    def save(self, file_path):
        """L∆∞u model"""
        data = {
            'n': self.n,
            'ngrams': dict(self.ngrams),
            'full_sentences': self.full_sentences,
            'vocab_size': self.vocab_size
        }
        
        with open(file_path, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"‚úì Model saved to {file_path}")
    
    def load(self, file_path):
        """Load model"""
        with open(file_path, 'rb') as f:
            data = pickle.load(f)
        
        self.n = data['n']
        self.ngrams = defaultdict(Counter, data['ngrams'])
        self.full_sentences = data['full_sentences']
        self.vocab_size = data['vocab_size']
        
        print(f"‚úì Model loaded from {file_path}")


# ========== SCRIPT TRAINING ==========
def train_ngram_model():
    """Script ƒë·ªÉ train v√† test model"""
    
    print("\n" + "="*70)
    print("üöÄ N-GRAM MODEL TRAINING")
    print("="*70)
    
    # ƒê∆∞·ªùng d·∫´n
    BASE_DIR = Path(__file__).parent.parent
    DATA_DIR = BASE_DIR / "data" / "processed"
    MODEL_DIR = BASE_DIR / "trained_models"
    
    # T·∫°o th∆∞ m·ª•c models n·∫øu ch∆∞a c√≥
    MODEL_DIR.mkdir(exist_ok=True)
    
    # Load data
    print(f"\nüìÇ Loading data...")
    
    with open(DATA_DIR / "train.json", 'r', encoding='utf-8') as f:
        train_data = json.load(f)
    
    with open(DATA_DIR / "test.json", 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    print(f"‚úì Train: {len(train_data)} samples")
    print(f"‚úì Test:  {len(test_data)} samples")
    
    # Train model
    model = NgramModel(n=3)  # Trigram
    model.train(train_data)
    
    # Test predictions
    print(f"\n{'‚îÄ'*60}")
    print("üß™ TEST PREDICTIONS")
    print(f"{'‚îÄ'*60}")
    
    test_inputs = [
        "ƒÉn qu·∫£",
        "c√≥ c√¥ng m√†i s·∫Øt",
        "g·∫ßn m·ª±c"
    ]
    
    for inp in test_inputs:
        print(f"\nüìù Input: '{inp}'")
        candidates = model.predict_multiple(inp, top_k=3)
        
        for i, cand in enumerate(candidates, 1):
            print(f"   {i}. {cand['text']}")
            print(f"      Confidence: {cand['confidence']:.1%} | Method: {cand['method']}")
    
    # Evaluate
    metrics = model.evaluate(test_data[:100])  # Test tr√™n 100 samples
    
    # Save model
    model_path = MODEL_DIR / "ngram_model.pkl"
    model.save(model_path)
    
    print(f"\n{'='*70}")
    print("‚úÖ TRAINING COMPLETE!")
    print("="*70)
    print(f"\nüìä Summary:")
    print(f"   ‚Ä¢ Vocabulary: {model.vocab_size:,} words")
    print(f"   ‚Ä¢ N-grams: {model.total_ngrams:,}")
    print(f"   ‚Ä¢ Accuracy: {metrics['accuracy']:.2%}")
    print(f"   ‚Ä¢ Model saved: {model_path}")
    print()


# ========== MAIN ==========
if __name__ == "__main__":
    train_ngram_model()