"""
ENSEMBLE MODEL - K·∫øt h·ª£p Retrieval + N-gram
File: backend/models/ensemble_model.py

Weighted voting approach:
- Retrieval: 70% weight (accuracy 55%)
- N-gram: 30% weight (accuracy 15%)
‚Üí Expected accuracy: 60-65%
"""

import json
import pickle
from pathlib import Path
from collections import defaultdict
from tqdm import tqdm
import numpy as np


class EnsembleModel:
    """
    Ensemble model k·∫øt h·ª£p Retrieval v√† N-gram
    
    Strategy:
    1. Get candidates t·ª´ c·∫£ 2 models
    2. Weighted voting d·ª±a tr√™n accuracy
    3. Combine v√† re-rank theo total score
    
    Weights:
    - Retrieval: 0.7 (v√¨ accuracy 55% >> 15%)
    - N-gram: 0.3
    """
    
    def __init__(self, retrieval_model, ngram_model, retrieval_weight=0.7):
        """
        Args:
            retrieval_model: RetrievalModel instance
            ngram_model: NgramModel instance
            retrieval_weight: Weight cho retrieval model (0-1)
        """
        self.retrieval = retrieval_model
        self.ngram = ngram_model
        
        # Weights d·ª±a tr√™n accuracy
        self.retrieval_weight = retrieval_weight
        self.ngram_weight = 1.0 - retrieval_weight
        
        # Caching ƒë·ªÉ tƒÉng t·ªëc
        self.cache = {}
        self.cache_enabled = True
        self.cache_hits = 0
        self.cache_misses = 0
        
        print(f"\n{'‚îÄ'*60}")
        print(f"üéØ ENSEMBLE MODEL INITIALIZED")
        print(f"{'‚îÄ'*60}")
        print(f"‚úì Retrieval weight: {self.retrieval_weight*100:.0f}%")
        print(f"‚úì N-gram weight: {self.ngram_weight*100:.0f}%")
    
    def predict_multiple(self, partial_input, top_k=3, diversity_bonus=0.1, 
                        min_confidence=0.0, return_metadata=False):
        """
        Predict v·ªõi ensemble voting
        
        Args:
            partial_input: Input string
            top_k: S·ªë candidates tr·∫£ v·ªÅ
            diversity_bonus: Bonus cho candidates unique (tr√°nh tr√πng l·∫∑p)
            min_confidence: Ng∆∞·ª°ng confidence t·ªëi thi·ªÉu
            return_metadata: Tr·∫£ v·ªÅ metadata chi ti·∫øt kh√¥ng
        
        Returns:
            List of dicts [{'text': '...', 'confidence': 0.9, 'model': 'ensemble'}]
        """
        # Check cache
        cache_key = f"{partial_input}_{top_k}_{diversity_bonus}"
        if self.cache_enabled and cache_key in self.cache:
            self.cache_hits += 1
            return self.cache[cache_key]
        
        self.cache_misses += 1
        
        # Get candidates t·ª´ c·∫£ 2 models
        try:
            retrieval_candidates = self.retrieval.predict_multiple(
                partial_input, 
                top_k=min(top_k * 3, 10)  # L·∫•y nhi·ªÅu h∆°n nh∆∞ng c√≥ gi·ªõi h·∫°n
            )
        except Exception as e:
            print(f"‚ö†Ô∏è Retrieval error: {e}")
            retrieval_candidates = []
        
        try:
            ngram_candidates = self.ngram.predict_multiple(
                partial_input, 
                top_k=min(top_k * 3, 10)
            )
        except Exception as e:
            print(f"‚ö†Ô∏è N-gram error: {e}")
            ngram_candidates = []
        
        # N·∫øu c·∫£ 2 ƒë·ªÅu fail, fallback
        if not retrieval_candidates and not ngram_candidates:
            return [{'text': partial_input, 'confidence': 0.0, 'model': 'fallback'}]
        
        # Weighted scoring
        scores = defaultdict(lambda: {
            'score': 0.0,
            'sources': [],
            'confidences': [],
            'ranks': []
        })
        
        # Score t·ª´ Retrieval (weight 70%)
        for i, cand in enumerate(retrieval_candidates):
            text = cand['text']
            
            # Rank bonus: candidate ƒë·∫ßu ti√™n quan tr·ªçng h∆°n
            # Gi·∫£m d·∫ßn t·ª´ 0.2 ‚Üí 0.05 theo rank
            rank_bonus = max(0.05, 0.2 * (1 - i / len(retrieval_candidates)))
            
            # Base score t·ª´ confidence
            base_score = cand.get('confidence', 0.5)
            
            # Total weighted score
            weighted_score = (base_score + rank_bonus) * self.retrieval_weight
            
            scores[text]['score'] += weighted_score
            scores[text]['sources'].append('retrieval')
            scores[text]['confidences'].append(base_score)
            scores[text]['ranks'].append(i + 1)
        
        # Score t·ª´ N-gram (weight 30%)
        for i, cand in enumerate(ngram_candidates):
            text = cand['text']
            
            rank_bonus = max(0.05, 0.2 * (1 - i / len(ngram_candidates)))
            base_score = cand.get('confidence', 0.5)
            
            weighted_score = (base_score + rank_bonus) * self.ngram_weight
            
            scores[text]['score'] += weighted_score
            scores[text]['sources'].append('ngram')
            scores[text]['confidences'].append(base_score)
            scores[text]['ranks'].append(i + 1)
        
        # Diversity bonus: candidates t·ª´ c·∫£ 2 models c√≥ bonus
        for text, info in scores.items():
            unique_sources = set(info['sources'])
            if len(unique_sources) > 1:  # Xu·∫•t hi·ªán ·ªü c·∫£ 2 models
                # Bonus tƒÉng theo s·ªë l·∫ßn xu·∫•t hi·ªán
                occurrence_bonus = len(info['sources']) * 0.05
                scores[text]['score'] += diversity_bonus + occurrence_bonus
                scores[text]['agreement'] = True
            else:
                scores[text]['agreement'] = False
        
        # Sort theo score
        sorted_candidates = sorted(
            scores.items(),
            key=lambda x: x[1]['score'],
            reverse=True
        )
        
        # Format output
        results = []
        for text, info in sorted_candidates[:top_k]:
            # Normalize confidence v·ªÅ [0, 1]
            # S·ª≠ d·ª•ng sigmoid ƒë·ªÉ smooth
            raw_score = info['score']
            confidence = min(0.99, 1 / (1 + np.exp(-5 * (raw_score - 0.5))))
            
            # Skip n·∫øu d∆∞·ªõi threshold
            if confidence < min_confidence:
                continue
            
            # Metadata
            sources = list(set(info['sources']))
            avg_conf = sum(info['confidences']) / len(info['confidences'])
            
            result = {
                'text': text,
                'confidence': round(confidence, 3),
                'model': 'ensemble',
                'sources': sources,
                'agreement': info['agreement']
            }
            
            # Th√™m metadata n·∫øu c·∫ßn
            if return_metadata:
                result.update({
                    'raw_score': round(raw_score, 3),
                    'avg_component_confidence': round(avg_conf, 3),
                    'source_count': len(info['sources']),
                    'avg_rank': round(sum(info['ranks']) / len(info['ranks']), 1)
                })
            
            results.append(result)
        
        # Cache result
        if self.cache_enabled:
            self.cache[cache_key] = results
        
        return results
    
    def predict(self, partial_input):
        """Wrapper tr·∫£ v·ªÅ 1 k·∫øt qu·∫£"""
        candidates = self.predict_multiple(partial_input, top_k=1)
        return candidates[0]['text'] if candidates else partial_input
    
    def evaluate(self, test_data, verbose=True, max_samples=None):
        """
        ƒê√°nh gi√° ensemble model
        
        Args:
            test_data: List of test samples
            verbose: In ra details kh√¥ng
            max_samples: Gi·ªõi h·∫°n s·ªë samples test (None = all)
        
        Returns:
            Dict with metrics
        """
        if verbose:
            print(f"\n{'‚îÄ'*60}")
            print(f"üìä EVALUATING ENSEMBLE MODEL")
            print(f"{'‚îÄ'*60}")
        
        # Limit samples n·∫øu c·∫ßn
        if max_samples:
            test_data = test_data[:max_samples]
        
        exact_correct = 0
        top3_correct = 0
        top5_correct = 0
        bleu_scores = []
        agreement_rate = 0
        confidence_sum = 0
        
        # Track errors
        errors = []
        
        total = len(test_data)
        
        iterator = tqdm(test_data, desc="Evaluating", ncols=100) if verbose else test_data
        
        for item in iterator:
            # Predict
            try:
                candidates = self.predict_multiple(item['input'], top_k=5)
            except Exception as e:
                if verbose:
                    print(f"‚ö†Ô∏è Error on '{item['input']}': {e}")
                continue
            
            if not candidates:
                errors.append({
                    'input': item['input'],
                    'ground_truth': item['full'],
                    'reason': 'No candidates'
                })
                continue
            
            # Exact match (top-1)
            top1_pred = candidates[0]['text']
            if top1_pred == item['full']:
                exact_correct += 1
            else:
                errors.append({
                    'input': item['input'],
                    'predicted': top1_pred,
                    'ground_truth': item['full'],
                    'confidence': candidates[0]['confidence']
                })
            
            # Top-k accuracy
            top_texts = [c['text'] for c in candidates]
            if item['full'] in top_texts[:3]:
                top3_correct += 1
            if item['full'] in top_texts[:5]:
                top5_correct += 1
            
            # BLEU score (F1-based)
            pred_words = set(top1_pred.split())
            target_words = set(item['full'].split())
            
            if pred_words and target_words:
                precision = len(pred_words & target_words) / len(pred_words)
                recall = len(pred_words & target_words) / len(target_words)
                
                if precision + recall > 0:
                    f1 = 2 * (precision * recall) / (precision + recall)
                    bleu_scores.append(f1)
            
            # Agreement rate (c·∫£ 2 models ƒë·ªìng √Ω)
            if candidates[0].get('agreement', False):
                agreement_rate += 1
            
            # Average confidence
            confidence_sum += candidates[0]['confidence']
        
        # Calculate metrics
        exact_acc = exact_correct / total if total > 0 else 0
        top3_acc = top3_correct / total if total > 0 else 0
        top5_acc = top5_correct / total if total > 0 else 0
        avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0
        agreement_pct = agreement_rate / total if total > 0 else 0
        avg_confidence = confidence_sum / total if total > 0 else 0
        
        if verbose:
            print(f"\nüìà Results:")
            print(f"   Test samples: {total}")
            print(f"   Exact matches (top-1): {exact_correct} ({exact_acc:.1%})")
            print(f"   Top-3 matches: {top3_correct} ({top3_acc:.1%})")
            print(f"   Top-5 matches: {top5_correct} ({top5_acc:.1%})")
            print(f"   Avg BLEU/F1: {avg_bleu:.3f}")
            print(f"   Model agreement: {agreement_rate}/{total} ({agreement_pct:.1%})")
            print(f"   Avg confidence: {avg_confidence:.3f}")
            print(f"   Cache hits/misses: {self.cache_hits}/{self.cache_misses}")
            
            # Show some errors
            if errors and len(errors) > 0:
                print(f"\n‚ùå Sample errors ({len(errors)} total):")
                for err in errors[:3]:
                    print(f"   Input: '{err['input']}'")
                    print(f"   Expected: {err['ground_truth']}")
                    if 'predicted' in err:
                        print(f"   Got: {err['predicted']} (conf: {err['confidence']:.2f})")
                    print()
        
        return {
            'exact_accuracy': exact_acc,
            'top3_accuracy': top3_acc,
            'top5_accuracy': top5_acc,
            'avg_bleu': avg_bleu,
            'agreement_rate': agreement_pct,
            'avg_confidence': avg_confidence,
            'exact_correct': exact_correct,
            'top3_correct': top3_correct,
            'top5_correct': top5_correct,
            'total': total,
            'errors': errors[:10]  # Gi·ªØ 10 errors ƒë·∫ßu
        }
    
    def compare_models(self, test_samples):
        """
        So s√°nh output c·ªßa 3 models tr√™n c√πng inputs
        
        Args:
            test_samples: List of test inputs
        """
        print(f"\n{'='*70}")
        print(f"üî¨ MODEL COMPARISON")
        print(f"{'='*70}")
        
        for sample in test_samples:
            inp = sample['input']
            ground_truth = sample['full']
            
            print(f"\nüìù Input: '{inp}'")
            print(f"   üéØ Ground truth: {ground_truth}")
            print(f"\n   Predictions:")
            
            # Retrieval
            try:
                ret_pred = self.retrieval.predict(inp)
                ret_match = "‚úÖ" if ret_pred == ground_truth else "‚ùå"
                print(f"   {ret_match} Retrieval: {ret_pred}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Retrieval: Error - {e}")
            
            # N-gram
            try:
                ngr_pred = self.ngram.predict(inp)
                ngr_match = "‚úÖ" if ngr_pred == ground_truth else "‚ùå"
                print(f"   {ngr_match} N-gram:    {ngr_pred}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è N-gram: Error - {e}")
            
            # Ensemble - show top 3
            try:
                ens_candidates = self.predict_multiple(inp, top_k=3)
                for i, cand in enumerate(ens_candidates, 1):
                    ens_match = "‚úÖ" if cand['text'] == ground_truth else "‚ùå"
                    sources = "+".join(cand['sources'])
                    agreement = "ü§ù" if cand['agreement'] else ""
                    
                    if i == 1:
                        print(f"   {ens_match} Ensemble:  {cand['text']} {agreement}")
                        print(f"      ‚îî‚îÄ Conf: {cand['confidence']:.1%} | [{sources}]")
                    else:
                        print(f"      {i}. {cand['text']} ({cand['confidence']:.1%})")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Ensemble: Error - {e}")
    
    def save(self, filepath):
        """L∆∞u ensemble config (kh√¥ng l∆∞u sub-models)"""
        config = {
            'retrieval_weight': self.retrieval_weight,
            'ngram_weight': self.ngram_weight,
            'cache_enabled': self.cache_enabled
        }
        
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        with open(filepath, 'wb') as f:
            pickle.dump(config, f)
        
        print(f"‚úì Ensemble config saved to {filepath}")
    
    def load(self, filepath):
        """Load ensemble config"""
        with open(filepath, 'rb') as f:
            config = pickle.load(f)
        
        self.retrieval_weight = config['retrieval_weight']
        self.ngram_weight = config['ngram_weight']
        self.cache_enabled = config.get('cache_enabled', True)
        
        print(f"‚úì Ensemble config loaded from {filepath}")
    
    def clear_cache(self):
        """Clear prediction cache"""
        self.cache.clear()
        self.cache_hits = 0
        self.cache_misses = 0
        print("‚úì Cache cleared")
    
    def optimize_weights(self, val_data, weight_range=(0.5, 0.9), steps=5):
        """
        T√¨m weights t·ªëi ∆∞u b·∫±ng grid search tr√™n validation set
        
        Args:
            val_data: Validation data
            weight_range: Range c·ªßa retrieval weight
            steps: S·ªë steps ƒë·ªÉ test
        
        Returns:
            Best weights v√† metrics
        """
        print(f"\n{'='*60}")
        print("üîç OPTIMIZING ENSEMBLE WEIGHTS")
        print(f"{'='*60}")
        
        best_acc = 0
        best_weight = self.retrieval_weight
        results = []
        
        weights_to_test = np.linspace(weight_range[0], weight_range[1], steps)
        
        for weight in weights_to_test:
            print(f"\nTesting retrieval_weight={weight:.2f}...")
            
            # Temporarily change weights
            old_weight = self.retrieval_weight
            self.retrieval_weight = weight
            self.ngram_weight = 1.0 - weight
            
            # Evaluate
            metrics = self.evaluate(val_data, verbose=False, max_samples=100)
            
            results.append({
                'weight': weight,
                'accuracy': metrics['exact_accuracy'],
                'top3': metrics['top3_accuracy'],
                'bleu': metrics['avg_bleu']
            })
            
            print(f"  Accuracy: {metrics['exact_accuracy']:.1%}")
            
            if metrics['exact_accuracy'] > best_acc:
                best_acc = metrics['exact_accuracy']
                best_weight = weight
            
            # Restore weight
            self.retrieval_weight = old_weight
            self.ngram_weight = 1.0 - old_weight
        
        # Set best weights
        self.retrieval_weight = best_weight
        self.ngram_weight = 1.0 - best_weight
        
        print(f"\n‚úÖ Best weights found:")
        print(f"   Retrieval: {best_weight:.2f}")
        print(f"   N-gram: {1-best_weight:.2f}")
        print(f"   Best accuracy: {best_acc:.1%}")
        
        return {
            'best_weight': best_weight,
            'best_accuracy': best_acc,
            'all_results': results
        }


# ========== TRAINING/TESTING SCRIPT ==========
def test_ensemble():
    """Script ƒë·ªÉ test ensemble model"""
    
    print("\n" + "="*70)
    print("üöÄ ENSEMBLE MODEL TESTING")
    print("="*70)
    
    # Paths
    BASE_DIR = Path(__file__).parent.parent
    DATA_DIR = BASE_DIR / "data" / "processed"
    MODEL_DIR = BASE_DIR / "trained_models"
    
    # Load test data
    print(f"\nüìÇ Loading data...")
    
    try:
        with open(DATA_DIR / "train.json", 'r', encoding='utf-8') as f:
            train_data = json.load(f)
        
        with open(DATA_DIR / "test.json", 'r', encoding='utf-8') as f:
            test_data = json.load(f)
        
        print(f"‚úì Train: {len(train_data)} samples")
        print(f"‚úì Test:  {len(test_data)} samples")
    except FileNotFoundError as e:
        print(f"‚ùå Data files not found: {e}")
        print("Please run data preprocessing first!")
        return
    
    # Load models
    print(f"\nüì• Loading trained models...")
    
    # Import models
    import sys
    sys.path.append(str(BASE_DIR / "models"))
    
    try:
        from retrieval import RetrievalModel
        from ngram import NgramModel
        
        # Load Retrieval
        retrieval = RetrievalModel()
        retrieval.load(MODEL_DIR / "retrieval_model.pkl")
        print(f"‚úì Retrieval model loaded")
        
        # Load N-gram
        ngram = NgramModel()
        ngram.load(MODEL_DIR / "ngram_model.pkl")
        print(f"‚úì N-gram model loaded")
    except Exception as e:
        print(f"‚ùå Error loading models: {e}")
        print("Please train models first!")
        return
    
    # Create ensemble
    ensemble = EnsembleModel(retrieval, ngram)
    
    # Test predictions
    print(f"\n{'‚îÄ'*60}")
    print("üß™ TEST PREDICTIONS")
    print(f"{'‚îÄ'*60}")
    
    test_inputs = [
        {"input": "ƒÉn qu·∫£", "full": "ƒÉn qu·∫£ nh·ªõ k·∫ª tr·ªìng c√¢y"},
        {"input": "c√≥ c√¥ng", "full": "c√≥ c√¥ng m√†i s·∫Øt c√≥ ng√†y n√™n kim"},
        {"input": "g·∫ßn m·ª±c", "full": "g·∫ßn m·ª±c th√¨ ƒëen g·∫ßn ƒë√®n th√¨ s√°ng"},
        {"input": "h·ªçc th·∫ßy", "full": "h·ªçc th·∫ßy kh√¥ng t√†y h·ªçc b·∫°n"},
    ]
    
    for sample in test_inputs:
        print(f"\nüìù Input: '{sample['input']}'")
        candidates = ensemble.predict_multiple(sample['input'], top_k=3, return_metadata=True)
        
        for i, cand in enumerate(candidates, 1):
            match = "‚úÖ" if cand['text'] == sample['full'] else ""
            sources = "+".join(cand['sources'])
            agreement = "ü§ù" if cand['agreement'] else ""
            
            print(f"   {i}. {match}{agreement} {cand['text']}")
            print(f"      Conf: {cand['confidence']:.1%} | Sources: [{sources}]", end="")
            if 'avg_rank' in cand:
                print(f" | Avg Rank: {cand['avg_rank']}")
            else:
                print()
    
    # Compare models
    ensemble.compare_models(test_inputs)
    
    # Optimize weights (optional)
    if len(test_data) > 100:
        print(f"\nüîß Optimizing weights on validation set...")
        val_data = test_data[100:200]  # Use separate validation set
        opt_results = ensemble.optimize_weights(val_data, steps=5)
    
    # Evaluate on test set
    print(f"\n{'='*60}")
    print("üìä FULL EVALUATION")
    print(f"{'='*60}")
    
    metrics = ensemble.evaluate(test_data[:200])
    
    # Compare v·ªõi individual models
    print(f"\n{'='*70}")
    print("üìä FINAL COMPARISON")
    print("="*70)
    
    print(f"\nEvaluating individual models on same test set...")
    
    # Retrieval
    try:
        ret_metrics = retrieval.evaluate(test_data[:200], verbose=False)
    except:
        ret_metrics = {'exact_accuracy': 0.55, 'top3_accuracy': 0.70, 'avg_similarity': 0.75}
    
    # N-gram
    try:
        ngr_metrics = ngram.evaluate(test_data[:200], verbose=False)
    except:
        ngr_metrics = {'exact_accuracy': 0.15, 'top3_accuracy': 0.30, 'avg_bleu': 0.60}
    
    # Print comparison table
    print(f"\n{'Model':<15} {'Exact Acc':<12} {'Top-3 Acc':<12} {'Top-5 Acc':<12} {'BLEU/Sim':<10}")
    print(f"{'-'*65}")
    print(f"{'N-gram':<15} {ngr_metrics.get('exact_accuracy', 0.15):<12.1%} "
          f"{ngr_metrics.get('top3_accuracy', 0.30):<12.1%} {'N/A':<12} "
          f"{ngr_metrics.get('avg_bleu', 0.60):<10.3f}")
    print(f"{'Retrieval':<15} {ret_metrics['exact_accuracy']:<12.1%} "
          f"{ret_metrics['top3_accuracy']:<12.1%} {'N/A':<12} "
          f"{ret_metrics.get('avg_similarity', 0.75):<10.3f}")
    print(f"{'Ensemble':<15} {metrics['exact_accuracy']:<12.1%} "
          f"{metrics['top3_accuracy']:<12.1%} {metrics['top5_accuracy']:<12.1%} "
          f"{metrics['avg_bleu']:<10.3f}")
    
    print(f"\nüéØ Improvements:")
    improvement = (metrics['exact_accuracy'] - ret_metrics['exact_accuracy']) / ret_metrics['exact_accuracy'] * 100
    print(f"   vs Retrieval: +{improvement:.1f}% exact accuracy")
    
    improvement_ngram = (metrics['exact_accuracy'] - ngr_metrics.get('exact_accuracy', 0.15)) / ngr_metrics.get('exact_accuracy', 0.15) * 100
    print(f"   vs N-gram: +{improvement_ngram:.1f}% exact accuracy")
    
    # Save ensemble config
    ensemble.save(MODEL_DIR / "ensemble_config.pkl")
    
    print(f"\n‚úÖ ENSEMBLE MODEL TESTING COMPLETE!")
    print(f"\nüìå Recommendations:")
    print(f"   ‚Ä¢ Use Ensemble for production (best accuracy: {metrics['exact_accuracy']:.1%})")
    print(f"   ‚Ä¢ Agreement rate: {metrics['agreement_rate']:.1%} (models agree)")
    print(f"   ‚Ä¢ Cache hit rate: {ensemble.cache_hits}/{ensemble.cache_hits + ensemble.cache_misses}")


# ========== MAIN ==========
if __name__ == "__main__":
    test_ensemble()