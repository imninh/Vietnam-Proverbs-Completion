"""
PREPROCESSING SCRIPT: L√ÄM S·∫†CH + T·∫†O SPLITS CHO RETRIEVAL MODEL
File: preprocessing.py

M·ª•c ƒë√≠ch: M·ªôt pipeline ho√†n ch·ªânh ƒë·ªÉ:
1. L√†m s·∫°ch dataset (x√≥a r·ªóng, chu·∫©n h√≥a, x√≥a tr√πng)
2. Chia train/val/test splits
3. L∆∞u output ·ªü d·∫°ng JSON + TXT

Ch·∫°y:
  python preprocessing.py

Input:  data/dataset_normalized.txt
Output: data/processed/
        ‚îú‚îÄ‚îÄ cleaned_dataset.txt
        ‚îú‚îÄ‚îÄ train.json / train.txt
        ‚îú‚îÄ‚îÄ val.json / val.txt
        ‚îî‚îÄ‚îÄ test.json / test.txt
"""

import json
import random
import sys
import re
import unicodedata
import hashlib
from pathlib import Path
from collections import Counter


class DataCleaner:
    """L√†m s·∫°ch dataset ca dao cho retrieval model"""
    
    def __init__(self):
        self.stats = {
            'original': 0,
            'empty_removed': 0,
            'too_short_removed': 0,
            'duplicate_removed': 0,
            'final': 0
        }
    
    def normalize_unicode(self, text):
        """Chu·∫©n h√≥a d·∫•u ti·∫øng Vi·ªát (NFC normalization)"""
        return unicodedata.normalize('NFC', text)
    
    def clean_text(self, text):
        """
        L√†m s·∫°ch text nh·∫π nh√†ng
        - X√≥a kho·∫£ng tr·∫Øng th·ª´a
        - Chu·∫©n h√≥a d·∫•u c√¢u
        - Gi·ªØ nguy√™n n·ªôi dung
        """
        # X√≥a BOM n·∫øu c√≥
        text = text.replace('\ufeff', '')
        
        # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
        text = ' '.join(text.split())
        
        # X√≥a d·∫•u c√°ch ƒë·∫ßu/cu·ªëi
        text = text.strip()
        
        return text
    
    def is_valid(self, text, min_words=3):
        """
        Ki·ªÉm tra c√¢u c√≥ h·ª£p l·ªá kh√¥ng
        - Kh√¥ng r·ªóng
        - √çt nh·∫•t min_words t·ª´
        - C√≥ √≠t nh·∫•t 50% ch·ªØ c√°i ti·∫øng Vi·ªát
        """
        if not text or len(text.strip()) == 0:
            return False
        
        words = text.split()
        if len(words) < min_words:
            return False
        
        # Ki·ªÉm tra t·ª∑ l·ªá ch·ªØ c√°i ti·∫øng Vi·ªát
        vietnamese_letters = re.findall(
            r'[a-z√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë]', 
            text.lower()
        )
        
        text_without_space = text.replace(' ', '').replace(',', '').replace('.', '')
        if len(text_without_space) == 0:
            return False
        
        letter_ratio = len(vietnamese_letters) / len(text_without_space)
        
        return letter_ratio >= 0.5
    
    def clean(self, input_file, min_words=3):
        """
        Pipeline l√†m s·∫°ch dataset
        
        Args:
            input_file: File input (dataset_normalized.txt)
            min_words: T·ªëi thi·ªÉu s·ªë t·ª´ cho m·ªói c√¢u
            
        Returns:
            List c√°c c√¢u ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch
        """
        
        print(f"\n{'‚îÄ'*70}")
        print("üßπ L√ÄM S·∫†CH DATASET")
        print(f"{'‚îÄ'*70}")
        
        # ƒê·ªçc file
        print(f"üìÇ ƒê·ªçc file: {input_file}")
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except FileNotFoundError:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {input_file}")
            return None
        
        self.stats['original'] = len(lines)
        print(f"‚úì ƒê·ªçc th√†nh c√¥ng {self.stats['original']:,} d√≤ng")
        
        # X·ª≠ l√Ω t·ª´ng d√≤ng
        print(f"\nB∆Ø·ªöC 1: L√†m s·∫°ch v√† l·ªçc d·ªØ li·ªáu")
        
        cleaned_lines = []
        
        for line in lines:
            # B·ªè d√≤ng r·ªóng
            if not line.strip():
                self.stats['empty_removed'] += 1
                continue
            
            # Chu·∫©n h√≥a Unicode
            line = self.normalize_unicode(line)
            
            # L√†m s·∫°ch text
            line = self.clean_text(line)
            
            # Ki·ªÉm tra h·ª£p l·ªá
            if not self.is_valid(line, min_words):
                self.stats['too_short_removed'] += 1
                continue
            
            cleaned_lines.append(line)
        
        print(f"‚úì X√≥a {self.stats['empty_removed']:,} d√≤ng r·ªóng")
        print(f"‚úì X√≥a {self.stats['too_short_removed']:,} c√¢u qu√° ng·∫Øn ho·∫∑c kh√¥ng h·ª£p l·ªá")
        print(f"  C√≤n l·∫°i: {len(cleaned_lines):,} c√¢u")
        
        # X√≥a tr√πng l·∫∑p
        print(f"\nB∆Ø·ªöC 2: X√≥a tr√πng l·∫∑p")
        
        seen = set()
        unique_lines = []
        
        for line in cleaned_lines:
            # So s√°nh kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng
            line_lower = line.lower()
            if line_lower not in seen:
                seen.add(line_lower)
                unique_lines.append(line)
            else:
                self.stats['duplicate_removed'] += 1
        
        print(f"‚úì X√≥a {self.stats['duplicate_removed']:,} c√¢u tr√πng l·∫∑p")
        print(f"  C√≤n l·∫°i: {len(unique_lines):,} c√¢u")
        
        self.stats['final'] = len(unique_lines)
        
        return unique_lines
    
    def print_report(self):
        """In b√°o c√°o t·ªïng h·ª£p"""
        print(f"\n{'‚îÄ'*70}")
        print("üìä B√ÅO C√ÅO L√ÄMM S·∫†CH")
        print("‚îÄ"*70)
        
        print(f"\nüìà Th·ªëng k√™:")
        print(f"   D√≤ng g·ªëc:              {self.stats['original']:>6,}")
        print(f"   ‚îú‚îÄ X√≥a r·ªóng:           {self.stats['empty_removed']:>6,}")
        print(f"   ‚îú‚îÄ X√≥a kh√¥ng h·ª£p l·ªá:   {self.stats['too_short_removed']:>6,}")
        print(f"   ‚îî‚îÄ X√≥a tr√πng l·∫∑p:      {self.stats['duplicate_removed']:>6,}")
        print(f"   {'‚îÄ'*35}")
        print(f"   ‚úÖ C√≤n l·∫°i:            {self.stats['final']:>6,}")
        
        if self.stats['original'] > 0:
            retention = (self.stats['final'] / self.stats['original']) * 100
            print(f"\nüìä T·ª∑ l·ªá gi·ªØ l·∫°i: {retention:.1f}%")
    
    def compute_checksum(self, data):
        """T√≠nh checksum c·ªßa cleaned data ƒë·ªÉ verify reproducibility"""
        text = '\n'.join(data)
        return hashlib.md5(text.encode()).hexdigest()


class DataSplitter:
    """Chia dataset cho retrieval model - train 100%, test l·∫•y 30% (c√≥ th·ªÉ tr√πng)"""
    
    def __init__(self, test_ratio=0.3):
        """
        Args:
            test_ratio: T·ª∑ l·ªá l·∫•y test set t·ª´ to√†n b·ªô data (m·∫∑c ƒë·ªãnh 30%)
                       Train s·∫Ω l√† to√†n b·ªô 100% data
                       Test ƒë∆∞·ª£c l·∫•y t·ª´ c√πng pool (c√≥ th·ªÉ tr√πng train)
        """
        assert 0 < test_ratio < 1
        self.test_ratio = test_ratio
        
        # ƒê·∫∑t seed ƒë·ªÉ reproducible
        random.seed(42)
    
    def split_data(self, data):
        """
        Chia dataset:
        - Train: 100% to√†n b·ªô data
        - Test: 30% l·∫•y ng·∫´u nhi√™n t·ª´ data (c√≥ th·ªÉ tr√πng train)
        """
        # Shuffle data ƒë·ªÉ l·∫•y test random
        data_shuffled = data.copy()
        random.shuffle(data_shuffled)
        
        n = len(data)
        test_size = int(n * self.test_ratio)
        
        return {
            'train': data,  # 100% to√†n b·ªô data
            'test': data_shuffled[:test_size]  # 30% l·∫•y t·ª´ data (c√≥ th·ªÉ tr√πng)
        }
    
    def analyze_lengths(self, data):
        """Ph√¢n t√≠ch ƒë·ªô d√†i c√¢u"""
        lengths = [len(sentence.split()) for sentence in data]
        return {
            'min': min(lengths) if lengths else 0,
            'max': max(lengths) if lengths else 0,
            'avg': sum(lengths) / len(lengths) if lengths else 0,
            'total': len(data)
        }
    
    def split_and_save(self, cleaned_data, output_dir):
        """
        Pipeline chia dataset v√† l∆∞u
        
        Args:
            cleaned_data: List c√°c c√¢u ƒë√£ l√†m s·∫°ch
            output_dir: Th∆∞ m·ª•c output
            
        Returns:
            Dict ch·ª©a train/val/test splits
        """
        print(f"\n{'‚îÄ'*70}")
        print("üì¶ T·∫†O DATASET TRAIN/TEST")
        print(f"{'‚îÄ'*70}")
        
        # Ph√¢n t√≠ch ƒë·ªô d√†i
        length_stats = self.analyze_lengths(cleaned_data)
        print(f"\nB∆Ø·ªöC 1: Ph√¢n t√≠ch ƒë·ªô d√†i")
        print(f"   S·ªë c√¢u:        {length_stats['total']:,}")
        print(f"   Ng·∫Øn nh·∫•t:     {length_stats['min']} t·ª´")
        print(f"   D√†i nh·∫•t:      {length_stats['max']} t·ª´")
        print(f"   Trung b√¨nh:    {length_stats['avg']:.1f} t·ª´")
        
        # Chia dataset
        print(f"\nB∆Ø·ªöC 2: Chia train/test")
        print(f"   Train: 100% (to√†n b·ªô data)")
        print(f"   Test:  {self.test_ratio*100:.0f}% (l·∫•y t·ª´ data, c√≥ th·ªÉ tr√πng train)")
        
        splits = self.split_data(cleaned_data)
        
        # T·∫°o th∆∞ m·ª•c output
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # L∆∞u files
        print(f"\nB∆Ø·ªöC 3: L∆∞u files")
        
        stats = {}
        for split_name, split_data in splits.items():
            # L∆∞u d·∫°ng JSON
            file_path = output_path / f"{split_name}.json"
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(split_data, f, ensure_ascii=False, indent=2)
            
            # Ph√¢n t√≠ch split n√†y
            stats[split_name] = self.analyze_lengths(split_data)
            
            print(f"   ‚úì {split_name:5s}: {len(split_data):>5,} c√¢u" +
                  f" (avg: {stats[split_name]['avg']:>5.1f} t·ª´)")
        
        # Ki·ªÉm tra ch·∫•t l∆∞·ª£ng
        self.validate_splits(splits)
        
        # C·∫£nh b√°o n·∫øu dataset qu√° nh·ªè
        if len(splits['train']) < 200:
            print(f"\n‚ö†Ô∏è  C·∫¢NH B√ÅO: Train set ch·ªâ c√≥ {len(splits['train'])} c√¢u")
            print(f"   Khuy·∫øn ngh·ªã:")
            print(f"   ‚Ä¢ Thu th·∫≠p th√™m ca dao (t·ªëi thi·ªÉu 500+ c√¢u)")
        elif len(splits['train']) < 500:
            print(f"\n‚ö†Ô∏è  L∆∞u √Ω: Train set c√≥ {len(splits['train'])} c√¢u")
            print(f"   C√≥ th·ªÉ c·∫ßn th√™m d·ªØ li·ªáu ƒë·ªÉ model ho·∫°t ƒë·ªông t·ªët h∆°n")
        else:
            print(f"\n‚úÖ Dataset size OK: {len(splits['train'])} c√¢u train")
        
        return splits
    
    def validate_splits(self, splits):
        """Ki·ªÉm tra ch·∫•t l∆∞·ª£ng split"""
        print(f"\nB∆Ø·ªöC 4: Ki·ªÉm tra ch·∫•t l∆∞·ª£ng")
        
        print(f"   ‚úÖ Train: 100% data")
        print(f"   ‚úÖ Test: {len(splits['test']):,} c√¢u (c√≥ th·ªÉ tr√πng train)")
        
        # Ki·ªÉm tra c√¢u kh√¥ng r·ªóng
        issues = 0
        for split_name, split_data in splits.items():
            for i, sentence in enumerate(split_data):
                if not sentence.strip():
                    print(f"   ‚ö†Ô∏è  {split_name}[{i}]: C√¢u r·ªóng!")
                    issues += 1
        
        if issues == 0:
            print(f"   ‚úÖ T·∫•t c·∫£ c√¢u ƒë·ªÅu h·ª£p l·ªá")
        else:
            print(f"   ‚ö†Ô∏è  T√¨m th·∫•y {issues} c√¢u c√≥ v·∫•n ƒë·ªÅ")
    
    def print_examples(self, splits):
        """Hi·ªÉn th·ªã v√≠ d·ª• t·ª´ m·ªói split"""
        print(f"\n{'‚îÄ'*70}")
        print("üìù V√ç D·ª§ T·ª™ M·ªñI SPLIT (3 c√¢u m·ªói lo·∫°i)")
        print("‚îÄ"*70)
        
        for split_name in ['train', 'test']:
            print(f"\n{split_name.upper()}:")
            for i, sentence in enumerate(splits[split_name][:3], 1):
                word_count = len(sentence.split())
                # C·∫Øt ng·∫Øn n·∫øu qu√° d√†i
                display = sentence if len(sentence) <= 60 else sentence[:57] + "..."
                print(f"   {i}. {display}")
                print(f"      ({word_count} t·ª´)")


# ========== MAIN PIPELINE ==========
if __name__ == "__main__":
    # ‚≠ê SET SEED NGAY T·ª™ ƒê·∫¶U ƒë·ªÉ ƒë·∫£m b·∫£o reproducibility
    random.seed(42)
    
    # ƒê∆∞·ªùng d·∫´n
    PROJECT_ROOT = Path(__file__).parent.parent.parent  # /NLP_v01/
    DATA_DIR = PROJECT_ROOT / "data"
    INPUT_FILE = DATA_DIR / "dataset_normalized.txt"
    OUTPUT_DIR = DATA_DIR / "processed"
    METADATA_FILE = OUTPUT_DIR / "metadata.json"
    
    # C·∫•u h√¨nh
    MIN_WORDS = 3
    TEST_RATIO = 0.3  # L·∫•y 30% l√†m test set (c√≥ th·ªÉ tr√πng train)
    
    print("\n" + "="*70)
    print("üöÄ PREPROCESSING PIPELINE (Cho Retrieval Model)")
    print("="*70)
    print(f"\nüì• Input:  {INPUT_FILE}")
    print(f"üì§ Output: {OUTPUT_DIR}/")
    print(f"\n‚öôÔ∏è  C·∫•u h√¨nh:")
    print(f"   ‚Ä¢ Min words per sentence: {MIN_WORDS}")
    print(f"   ‚Ä¢ Train: 100% (to√†n b·ªô data)")
    print(f"   ‚Ä¢ Test:  {TEST_RATIO*100:.0f}% (l·∫•y t·ª´ data, c√≥ th·ªÉ tr√πng train)")
    
    # B∆∞·ªõc 1: L√†m s·∫°ch dataset
    print(f"\n{'='*70}")
    print("PHASE 1: DATA CLEANING")
    print("="*70)
    
    cleaner = DataCleaner()
    cleaned_data = cleaner.clean(
        input_file=INPUT_FILE,
        min_words=MIN_WORDS
    )
    
    if cleaned_data is None:
        print(f"\n‚ùå L√†m s·∫°ch th·∫•t b·∫°i!")
        sys.exit(1)
    
    # L∆∞u cleaned data
    cleaned_file = OUTPUT_DIR / "cleaned_dataset.txt"
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    with open(cleaned_file, 'w', encoding='utf-8') as f:
        for line in cleaned_data:
            f.write(line + '\n')
    print(f"\n‚úì ƒê√£ l∆∞u cleaned data: {cleaned_file}")
    
    # T√≠nh checksum c·ªßa cleaned data
    checksum = cleaner.compute_checksum(cleaned_data)
    print(f"\nüîê Checksum cleaned data: {checksum}")
    
    # B∆∞·ªõc 2: Chia train/test
    print(f"\n{'='*70}")
    print("PHASE 2: CREATE SPLITS")
    print("="*70)
    
    splitter = DataSplitter(test_ratio=TEST_RATIO)
    
    splits = splitter.split_and_save(
        cleaned_data=cleaned_data,
        output_dir=OUTPUT_DIR
    )
    
    # T√≥m t·∫Øt
    print(f"\n{'='*70}")
    print("‚úÖ PREPROCESSING HO√ÄN TH√ÄNH!")
    print("="*70)
    print(f"\nüìä K·∫øt qu·∫£:")
    print(f"   ‚Ä¢ Train: {len(splits['train']):,} c√¢u (100%)")
    print(f"   ‚Ä¢ Test:  {len(splits['test']):,} c√¢u ({TEST_RATIO*100:.0f}%, c√≥ th·ªÉ tr√πng train)")
    print(f"\nüìÅ Output files:")
    print(f"   ‚úì {cleaned_file}")
    print(f"   ‚úì {OUTPUT_DIR / 'train.json'}")
    print(f"   ‚úì {OUTPUT_DIR / 'test.json'}")
    
    # L∆∞u metadata ƒë·ªÉ verify reproducibility
    metadata = {
        'timestamp': str(Path(cleaned_file).stat().st_mtime),
        'cleaned_data_checksum': checksum,
        'total_cleaned_sentences': len(cleaned_data),
        'train_size': len(splits['train']),
        'test_size': len(splits['test']),
        'test_ratio': TEST_RATIO,
        'min_words': MIN_WORDS,
        'random_seed': 42
    }
    
    with open(METADATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    print(f"   ‚úì {METADATA_FILE}")
    
    print(f"\nüîê REPRODUCIBILITY:")
    print(f"   ‚Ä¢ Random seed: 42 (c·ªë ƒë·ªãnh)")
    print(f"   ‚Ä¢ Cleaned data checksum: {checksum}")
    print(f"   ‚Ä¢ Metadata saved: {METADATA_FILE}")
    print(f"\n   ‚ÑπÔ∏è  C√°c l·∫ßn ch·∫°y ti·∫øp theo s·∫Ω t·∫°o k·∫øt qu·∫£ gi·ªëng h·ªát")
    print(f"       n·∫øu input file kh√¥ng thay ƒë·ªïi!")
    
    print(f"\nüìå B∆∞·ªõc ti·∫øp theo: Train retrieval model v·ªõi 100% d·ªØ li·ªáu")
    print()
