"""
SCRIPT 3: Táº O TRAIN/VAL/TEST SPLIT (CHO RETRIEVAL)
File: 3_create_splits_simple.py

Má»¥c Ä‘Ã­ch: Chia dataset thÃ nh train/val/test cho retrieval model
- Má»—i cÃ¢u giá»¯ nguyÃªn dáº¡ng Ä‘áº§y Ä‘á»§
- KhÃ´ng táº¡o input/target variants
- Chá»‰ lÆ°u danh sÃ¡ch cÃ¢u Ä‘áº§y Ä‘á»§

Cháº¡y:
  python 3_create_splits_simple.py
"""

import json
import random
import sys
from pathlib import Path
from collections import Counter


class SimpleDatasetSplitter:
    """Chia dataset Ä‘Æ¡n giáº£n cho retrieval model"""
    
    def __init__(self, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
        assert abs((train_ratio + val_ratio + test_ratio) - 1.0) < 0.01
        self.train_ratio = train_ratio
        self.val_ratio = val_ratio
        self.test_ratio = test_ratio
        
        # Äáº·t seed Ä‘á»ƒ reproducible
        random.seed(42)
    
    def split_data(self, data):
        """Chia dataset theo tá»· lá»‡ train/val/test"""
        random.shuffle(data)
        
        n = len(data)
        train_end = int(n * self.train_ratio)
        val_end = train_end + int(n * self.val_ratio)
        
        return {
            'train': data[:train_end],
            'val': data[train_end:val_end],
            'test': data[val_end:]
        }
    
    def analyze_lengths(self, data):
        """PhÃ¢n tÃ­ch Ä‘á»™ dÃ i cÃ¢u"""
        lengths = [len(sentence.split()) for sentence in data]
        return {
            'min': min(lengths) if lengths else 0,
            'max': max(lengths) if lengths else 0,
            'avg': sum(lengths) / len(lengths) if lengths else 0,
            'total': len(data)
        }
    
    def create(self, input_file, output_dir):
        """
        Pipeline chÃ­nh
        
        Args:
            input_file: File Ä‘Ã£ lÃ m sáº¡ch (cleaned_dataset.txt)
            output_dir: ThÆ° má»¥c output
        """
        print("\n" + "="*70)
        print("ğŸ“¦ Táº O DATASET TRAIN/VAL/TEST (CHO RETRIEVAL)")
        print("="*70)
        
        # Äá»c file
        print(f"\nğŸ“‚ Äá»c file: {input_file}")
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                sentences = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {input_file}")
            print(f"   HÃ£y cháº¡y 2_clean_data_simple.py trÆ°á»›c!")
            return False
        
        print(f"âœ“ Äá»c thÃ nh cÃ´ng {len(sentences):,} cÃ¢u ca dao")
        
        # PhÃ¢n tÃ­ch Ä‘á»™ dÃ i
        length_stats = self.analyze_lengths(sentences)
        print(f"\nğŸ“Š PhÃ¢n tÃ­ch Ä‘á»™ dÃ i:")
        print(f"   Sá»‘ cÃ¢u:        {length_stats['total']:,}")
        print(f"   Ngáº¯n nháº¥t:     {length_stats['min']} tá»«")
        print(f"   DÃ i nháº¥t:      {length_stats['max']} tá»«")
        print(f"   Trung bÃ¬nh:    {length_stats['avg']:.1f} tá»«")
        
        # Chia dataset
        print(f"\n{'â”€'*70}")
        print("BÆ¯á»šC 1: Chia train/val/test")
        print(f"   Train: {self.train_ratio*100:.0f}%")
        print(f"   Val:   {self.val_ratio*100:.0f}%")
        print(f"   Test:  {self.test_ratio*100:.0f}%")
        
        splits = self.split_data(sentences)
        
        # Táº¡o thÆ° má»¥c output
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # LÆ°u files
        print(f"\n{'â”€'*70}")
        print("BÆ¯á»šC 2: LÆ°u files")
        
        stats = {}
        for split_name, split_data in splits.items():
            # LÆ°u dáº¡ng list Ä‘Æ¡n giáº£n
            file_path = output_path / f"{split_name}.json"
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(split_data, f, ensure_ascii=False, indent=2)
            
            # LÆ°u cáº£ dáº¡ng text Ä‘á»ƒ dá»… Ä‘á»c
            text_path = output_path / f"{split_name}.txt"
            with open(text_path, 'w', encoding='utf-8') as f:
                for sentence in split_data:
                    f.write(sentence + '\n')
            
            # PhÃ¢n tÃ­ch split nÃ y
            stats[split_name] = self.analyze_lengths(split_data)
            
            print(f"   âœ“ {split_name:5s}: {len(split_data):>5,} cÃ¢u" +
                  f" (avg: {stats[split_name]['avg']:>5.1f} tá»«)")
        
        # Kiá»ƒm tra cháº¥t lÆ°á»£ng
        self.validate_splits(splits)
        
        # Hiá»ƒn thá»‹ vÃ­ dá»¥
        self.print_examples(splits)
        
        # Cáº£nh bÃ¡o náº¿u dataset quÃ¡ nhá»
        if len(splits['train']) < 200:
            print(f"\nâš ï¸  Cáº¢NH BÃO: Train set chá»‰ cÃ³ {len(splits['train'])} cÃ¢u")
            print(f"   Model cÃ³ thá»ƒ khÃ´ng há»c tá»‘t. Khuyáº¿n nghá»‹:")
            print(f"   â€¢ Thu tháº­p thÃªm ca dao (tá»‘i thiá»ƒu 500+ cÃ¢u)")
        elif len(splits['train']) < 500:
            print(f"\nâš ï¸  LÆ°u Ã½: Train set cÃ³ {len(splits['train'])} cÃ¢u")
            print(f"   CÃ³ thá»ƒ cáº§n thÃªm dá»¯ liá»‡u Ä‘á»ƒ model hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n")
        else:
            print(f"\nâœ… Dataset size OK: {len(splits['train'])} cÃ¢u train")
        
        print(f"\n{'='*70}")
        print("âœ… HOÃ€N THÃ€NH!")
        print("="*70)
        print(f"\nğŸ“Œ BÆ°á»›c tiáº¿p theo: Train retrieval model")
        print(f"   python retrieval.py\n")
        
        return True
    
    def validate_splits(self, splits):
        """Kiá»ƒm tra cháº¥t lÆ°á»£ng split"""
        print(f"\n{'â”€'*70}")
        print("BÆ¯á»šC 3: Kiá»ƒm tra cháº¥t lÆ°á»£ng")
        
        # Kiá»ƒm tra khÃ´ng cÃ³ cÃ¢u trÃ¹ng giá»¯a train/val/test
        train_set = set(s.lower() for s in splits['train'])
        val_set = set(s.lower() for s in splits['val'])
        test_set = set(s.lower() for s in splits['test'])
        
        overlap_train_val = train_set & val_set
        overlap_train_test = train_set & test_set
        overlap_val_test = val_set & test_set
        
        total_overlap = len(overlap_train_val) + len(overlap_train_test) + len(overlap_val_test)
        
        if total_overlap == 0:
            print(f"   âœ… KhÃ´ng cÃ³ cÃ¢u trÃ¹ng giá»¯a train/val/test")
        else:
            print(f"   âš ï¸  PhÃ¡t hiá»‡n {total_overlap} cÃ¢u trÃ¹ng!")
            if overlap_train_val:
                print(f"      â€¢ Train-Val: {len(overlap_train_val)} cÃ¢u")
            if overlap_train_test:
                print(f"      â€¢ Train-Test: {len(overlap_train_test)} cÃ¢u")
            if overlap_val_test:
                print(f"      â€¢ Val-Test: {len(overlap_val_test)} cÃ¢u")
        
        # Kiá»ƒm tra cÃ¢u khÃ´ng rá»—ng
        issues = 0
        for split_name, split_data in splits.items():
            for i, sentence in enumerate(split_data):
                if not sentence.strip():
                    print(f"   âš ï¸  {split_name}[{i}]: CÃ¢u rá»—ng!")
                    issues += 1
        
        if issues == 0:
            print(f"   âœ… Táº¥t cáº£ cÃ¢u Ä‘á»u há»£p lá»‡")
        else:
            print(f"   âš ï¸  TÃ¬m tháº¥y {issues} cÃ¢u cÃ³ váº¥n Ä‘á»")
    
    def print_examples(self, splits):
        """Hiá»ƒn thá»‹ vÃ­ dá»¥ tá»« má»—i split"""
        print(f"\n{'â”€'*70}")
        print("ğŸ“ VÃ Dá»¤ Tá»ª Má»–I SPLIT (3 cÃ¢u má»—i loáº¡i)")
        print("â”€"*70)
        
        for split_name in ['train', 'val', 'test']:
            print(f"\n{split_name.upper()}:")
            for i, sentence in enumerate(splits[split_name][:3], 1):
                word_count = len(sentence.split())
                # Cáº¯t ngáº¯n náº¿u quÃ¡ dÃ i
                display = sentence if len(sentence) <= 60 else sentence[:57] + "..."
                print(f"   {i}. {display}")
                print(f"      ({word_count} tá»«)")


# ========== MAIN ==========
if __name__ == "__main__":
    # ÄÆ°á»ng dáº«n - Báº N Cáº¦N Sá»¬A Äá»”I CHá»– NÃ€Y
    BASE_DIR = Path(__file__).parent.parent
    INPUT_FILE = BASE_DIR / "data" / "processed" / "cleaned_dataset.txt"
    OUTPUT_DIR = BASE_DIR / "data" / "processed"
    
    # Cáº¥u hÃ¬nh
    TRAIN_RATIO = 0.7
    VAL_RATIO = 0.15
    TEST_RATIO = 0.15
    
    print("\nğŸš€ Báº®T Äáº¦U Táº O DATASET SPLITS")
    print(f"ğŸ“¥ Input:  {INPUT_FILE}")
    print(f"ğŸ“¤ Output: {OUTPUT_DIR}/")
    print(f"âš™ï¸  Cáº¥u hÃ¬nh:")
    print(f"   â€¢ Train/Val/Test: {TRAIN_RATIO}/{VAL_RATIO}/{TEST_RATIO}")
    
    # Cháº¡y splitter
    splitter = SimpleDatasetSplitter(
        train_ratio=TRAIN_RATIO,
        val_ratio=VAL_RATIO,
        test_ratio=TEST_RATIO
    )
    
    success = splitter.create(
        input_file=INPUT_FILE,
        output_dir=OUTPUT_DIR
    )
    
    if not success:
        sys.exit(1)