"""
RETRIEVAL MODEL - PHI√äN B·∫¢N ƒê∆†N GI·∫¢N
File: retrieval_simple.py

M√¥ t·∫£:
- S·ª≠ d·ª•ng BM25 ranking (t·ªët h∆°n TF-IDF cosine)
- Prefix matching boost: +10 ƒëi·ªÉm n·∫øu c√¢u b·∫Øt ƒë·∫ßu ch√≠nh x√°c b·∫±ng query
- Tr·∫£ v·ªÅ c√¢u ƒë·∫ßy ƒë·ªß (kh√¥ng c·∫Øt input/target)

C√°ch d√πng:
1. Train: python retrieval_simple.py
2. Trong code kh√°c:
   from retrieval_simple import RetrievalModel
   model = RetrievalModel()
   model.load('retrieval_model.pkl')
   result = model.predict("ƒÉn qu·∫£ nh·ªõ")
   # Output: "ƒÉn qu·∫£ nh·ªõ k·∫ª tr·ªìng c√¢y. u·ªëng n∆∞·ªõc nh·ªõ ng∆∞·ªùi ƒë√†o gi·∫øng."
"""

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import numpy as np
import json
import pickle
from pathlib import Path


class RetrievalModel:
    """
    Retrieval model s·ª≠ d·ª•ng BM25 v·ªõi prefix matching boost
    
    ∆Øu ƒëi·ªÉm:
    - BM25: T·ªët h∆°n TF-IDF cho text retrieval
    - Prefix boost: ∆Øu ti√™n c√¢u b·∫Øt ƒë·∫ßu gi·ªëng query
    - ƒê∆°n gi·∫£n: Kh√¥ng c·∫ßn training ph·ª©c t·∫°p
    - Nhanh: Ch·ªâ c·∫ßn vector search
    """
    
    def __init__(self, analyzer='char_wb', ngram_range=(2, 4), max_features=10000, 
                 bm25_k1=1.5, bm25_b=0.75):
        """
        Args:
            analyzer: 'char_wb' = character n-grams trong word boundaries
            ngram_range: (2, 4) = bigram ƒë·∫øn 4-gram
            max_features: K√≠ch th∆∞·ªõc vocabulary
            bm25_k1: TF saturation parameter (1.2-2.0)
            bm25_b: Length normalization (0.75 typical)
        """
        self.vectorizer = TfidfVectorizer(
            analyzer=analyzer,
            ngram_range=ngram_range,
            max_features=max_features,
            lowercase=True,
            strip_accents=None,  # Gi·ªØ d·∫•u ti·∫øng Vi·ªát
        )
        
        self.database = []  # Danh s√°ch c√¢u ƒë·∫ßy ƒë·ªß
        self.term_freqs = None  # Document-term matrix
        self.idf = None  # IDF values
        self.doc_lengths = None  # ƒê·ªô d√†i m·ªói doc
        self.avg_doc_len = 0
        self.bm25_k1 = bm25_k1
        self.bm25_b = bm25_b
        self.is_trained = False
    
    def train(self, train_data):
        """
        Train model
        
        Args:
            train_data: List of strings (danh s√°ch c√¢u ƒë·∫ßy ƒë·ªß)
                       VD: ["ƒÉn qu·∫£ nh·ªõ k·∫ª tr·ªìng c√¢y", "c√≥ c√¥ng m√†i s·∫Øt", ...]
        """
        print(f"\n{'‚îÄ'*60}")
        print(f"üîÑ TRAINING RETRIEVAL MODEL (BM25)")
        print(f"{'‚îÄ'*60}")
        
        # L·ªçc tr√πng l·∫∑p
        seen = set()
        for sentence in train_data:
            if sentence not in seen:
                self.database.append(sentence)
                seen.add(sentence)
        
        print(f"üìä Database: {len(self.database):,} c√¢u unique")
        
        # Vectorize ƒë·ªÉ l·∫•y term frequencies
        count_vec = CountVectorizer(
            analyzer=self.vectorizer.analyzer,
            ngram_range=self.vectorizer.ngram_range,
            max_features=self.vectorizer.max_features,
            lowercase=True,
            strip_accents=None,
        )
        self.term_freqs = count_vec.fit_transform(self.database)
        
        # T√≠nh IDF
        self.idf = self.vectorizer.fit(self.database).idf_
        
        # T√≠nh ƒë·ªô d√†i doc (s·ªë t·ª´)
        self.doc_lengths = np.array([len(doc.split()) for doc in self.database])
        self.avg_doc_len = np.mean(self.doc_lengths) if len(self.doc_lengths) > 0 else 0
        
        print(f"‚úì Term matrix shape: {self.term_freqs.shape}")
        print(f"‚úì Vocabulary size: {len(count_vec.vocabulary_):,}")
        print(f"‚úì Average doc length: {self.avg_doc_len:.1f} t·ª´")
        
        self.is_trained = True
    
    def compute_bm25_scores(self, query):
        """
        T√≠nh BM25 scores cho t·∫•t c·∫£ documents
        
        BM25 formula:
        score(d, q) = Œ£ IDF(t) √ó [TF(t,d) √ó (k1 + 1)] / [TF(t,d) + k1 √ó (1 - b + b √ó len(d)/avg_len)]
        """
        if not self.is_trained:
            raise ValueError("Model ch∆∞a ƒë∆∞·ª£c train!")
        
        # Transform query
        from sklearn.feature_extraction.text import CountVectorizer
        count_vec = CountVectorizer(vocabulary=self.vectorizer.vocabulary_)
        query_tf = count_vec.fit_transform([query]).toarray()[0]
        
        scores = np.zeros(len(self.database))
        
        # T√≠nh score cho m·ªói term trong query
        for term_idx in np.nonzero(query_tf)[0]:
            # TF trong docs cho term n√†y
            tf_docs = self.term_freqs[:, term_idx].toarray().flatten()
            
            # IDF c·ªßa term
            idf_term = self.idf[term_idx]
            
            # BM25 score
            numerator = tf_docs * (self.bm25_k1 + 1)
            denominator = tf_docs + self.bm25_k1 * (
                1 - self.bm25_b + self.bm25_b * (self.doc_lengths / self.avg_doc_len)
            )
            term_scores = idf_term * (numerator / denominator)
            
            scores += term_scores
        
        return scores
    
    def retrieve(self, query, top_k=10):
        """
        T√¨m top-k c√¢u gi·ªëng nh·∫•t v·ªõi query
        
        Args:
            query: Input string (VD: "ƒÉn qu·∫£")
            top_k: S·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ
        
        Returns:
            List of (sentence, score) tuples
        """
        # T√≠nh BM25 scores
        scores = self.compute_bm25_scores(query.lower())
        
        # Th√™m prefix matching boost
        query_words = query.lower().split()
        query_len = len(query_words)
        
        for i, sentence in enumerate(self.database):
            sent_words = sentence.lower().split()
            # N·∫øu sentence b·∫Øt ƒë·∫ßu ch√≠nh x√°c b·∫±ng query ‚Üí boost
            if sent_words[:query_len] == query_words:
                scores[i] += 10.0
        
        # L·∫•y top-k
        top_indices = np.argsort(scores)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if scores[idx] > 0:
                results.append((self.database[idx], float(scores[idx])))
        
        return results
    
    def predict(self, partial_input, top_k=1):
        """
        D·ª± ƒëo√°n c√¢u ƒë·∫ßy ƒë·ªß t·ª´ input m·ªôt ph·∫ßn
        
        Args:
            partial_input: Input string (VD: "ƒÉn qu·∫£")
            top_k: S·ªë l∆∞·ª£ng k·∫øt qu·∫£ (m·∫∑c ƒë·ªãnh 1 = ch·ªâ tr·∫£ v·ªÅ c√¢u t·ªët nh·∫•t)
        
        Returns:
            N·∫øu top_k=1: String (c√¢u ƒë·∫ßy ƒë·ªß)
            N·∫øu top_k>1: List of dicts v·ªõi info chi ti·∫øt
        """
        retrieved = self.retrieve(partial_input, top_k=top_k)
        
        if not retrieved:
            # Fallback: tr·∫£ v·ªÅ random
            import random
            return random.choice(self.database) if self.database else partial_input
        
        if top_k == 1:
            return retrieved[0][0]  # Ch·ªâ tr·∫£ v·ªÅ text
        else:
            # Tr·∫£ v·ªÅ list v·ªõi details
            max_score = max([s[1] for s in retrieved])
            results = []
            for sentence, score in retrieved:
                confidence = min(0.99, score / max_score) if max_score > 0 else 0.05
                results.append({
                    'text': sentence,
                    'score': round(score, 3),
                    'confidence': round(confidence, 3)
                })
            return results
    
    def predict_multiple(self, partial_input, top_k=3):
        """
        Tr·∫£ v·ªÅ nhi·ªÅu candidates cho API
        Gi·ªëng predict nh∆∞ng lu√¥n tr·∫£ v·ªÅ list
        """
        return self.predict(partial_input, top_k=top_k)
    
    def evaluate(self, test_data, test_queries=None):
        """
        ƒê√°nh gi√° model
        
        Args:
            test_data: List of full sentences ƒë·ªÉ test
            test_queries: (Optional) List of tuples (query, expected_sentence)
                         N·∫øu None, s·∫Ω t·ª± t·∫°o queries t·ª´ test_data
        """
        print(f"\n{'‚îÄ'*60}")
        print(f"üìä ƒê√ÅNH GI√Å MODEL")
        print(f"{'‚îÄ'*60}")
        
        # N·∫øu kh√¥ng c√≥ test_queries, t·ª± t·∫°o
        if test_queries is None:
            test_queries = []
            for sentence in test_data[:50]:  # Test 50 c√¢u ƒë·∫ßu
                words = sentence.split()
                if len(words) >= 3:
                    # L·∫•y 2-4 t·ª´ ƒë·∫ßu l√†m query
                    query_len = min(len(words) // 2, 4)
                    query = ' '.join(words[:query_len])
                    test_queries.append((query, sentence))
        
        print(f"Test queries: {len(test_queries)}")
        
        exact_correct = 0
        top3_correct = 0
        
        for query, expected in test_queries:
            # Predict top-3
            results = self.predict(query, top_k=3)
            
            # Check exact match (top-1)
            if results[0]['text'] == expected:
                exact_correct += 1
            
            # Check top-3
            top3_texts = [r['text'] for r in results]
            if expected in top3_texts:
                top3_correct += 1
        
        total = len(test_queries)
        exact_acc = exact_correct / total if total > 0 else 0
        top3_acc = top3_correct / total if total > 0 else 0
        
        print(f"‚úì Exact match (top-1): {exact_correct}/{total} = {exact_acc:.1%}")
        print(f"‚úì Top-3 match:         {top3_correct}/{total} = {top3_acc:.1%}")
        
        return {
            'exact_accuracy': exact_acc,
            'top3_accuracy': top3_acc,
            'exact_correct': exact_correct,
            'top3_correct': top3_correct,
            'total': total
        }
    
    def save(self, file_path):
        """L∆∞u model"""
        data = {
            'vectorizer': self.vectorizer,
            'database': self.database,
            'term_freqs': self.term_freqs,
            'idf': self.idf,
            'doc_lengths': self.doc_lengths,
            'avg_doc_len': self.avg_doc_len,
            'bm25_k1': self.bm25_k1,
            'bm25_b': self.bm25_b
        }
        
        with open(file_path, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"‚úì Model saved: {file_path}")
    
    def load(self, file_path):
        """Load model"""
        with open(file_path, 'rb') as f:
            data = pickle.load(f)
        
        self.vectorizer = data['vectorizer']
        self.database = data['database']
        self.term_freqs = data['term_freqs']
        self.idf = data['idf']
        self.doc_lengths = data['doc_lengths']
        self.avg_doc_len = data['avg_doc_len']
        self.bm25_k1 = data['bm25_k1']
        self.bm25_b = data['bm25_b']
        self.is_trained = True
        
        print(f"‚úì Model loaded: {file_path}")


# ========== SCRIPT TRAINING ==========
def train_and_test():
    """Script ch√≠nh ƒë·ªÉ train v√† test model"""
    
    print("\n" + "="*70)
    print("üöÄ RETRIEVAL MODEL TRAINING")
    print("="*70)
    
    # ƒê∆∞·ªùng d·∫´n - B·∫†N C·∫¶N S·ª¨A CH·ªñ N√ÄY
    BASE_DIR = Path(__file__).parent.parent
    TRAIN_FILE = BASE_DIR / "data" / "processed" / "train.json"
    TEST_FILE = BASE_DIR / "data" / "processed" / "test.json"
    MODEL_FILE = BASE_DIR / "trained_models" / "retrieval_model.pkl"
    # Load data
    print(f"\nüìÇ Loading data...")
    
    try:
        with open(TRAIN_FILE, 'r', encoding='utf-8') as f:
            train_data = json.load(f)
        
        with open(TEST_FILE, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
    except FileNotFoundError as e:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {e}")
        print(f"\nüí° H√£y ch·∫°y c√°c script theo th·ª© t·ª±:")
        print(f"   1. python 2_clean_data_simple.py")
        print(f"   2. python 3_create_splits_simple.py")
        print(f"   3. python retrieval_simple.py")
        return
    
    print(f"‚úì Train: {len(train_data):,} c√¢u")
    print(f"‚úì Test:  {len(test_data):,} c√¢u")
    
    # Train model
    model = RetrievalModel()
    model.train(train_data)
    
    # Test predictions
    print(f"\n{'‚îÄ'*60}")
    print("üß™ TEST PREDICTIONS")
    print(f"{'‚îÄ'*60}")
    
    test_inputs = [
        "ƒÉn",
        "ƒÉn qu·∫£",
        "ƒÉn qu·∫£ nh·ªõ",
        "c√≥ c√¥ng",
        "c√≥ c√¥ng m√†i s·∫Øt",
        "g·∫ßn m·ª±c",
        "h·ªçc th·∫ßy kh√¥ng"
    ]
    
    for inp in test_inputs:
        print(f"\nüìù Input: '{inp}'")
        
        # Predict top-3
        results = model.predict(inp, top_k=3)
        
        for i, result in enumerate(results, 1):
            print(f"   {i}. {result['text']}")
            print(f"      üìä Score: {result['score']:.3f} | Confidence: {result['confidence']:.1%}")
    
    # Evaluate
    metrics = model.evaluate(test_data)
    
    # Save model
    model.save(MODEL_FILE)
    
    print(f"\n{'='*70}")
    print("‚úÖ HO√ÄN TH√ÄNH!")
    print("="*70)
    print(f"\nüìä T√≥m t·∫Øt:")
    print(f"   ‚Ä¢ Database: {len(model.database):,} c√¢u")
    print(f"   ‚Ä¢ Vocabulary: {model.term_freqs.shape[1]:,} features")
    print(f"   ‚Ä¢ Exact accuracy: {metrics['exact_accuracy']:.1%}")
    print(f"   ‚Ä¢ Top-3 accuracy: {metrics['top3_accuracy']:.1%}")
    print(f"   ‚Ä¢ Model saved: {MODEL_FILE}")
    
    print(f"\nüí° C√°ch s·ª≠ d·ª•ng model:")
    print(f"   from retrieval_simple import RetrievalModel")
    print(f"   model = RetrievalModel()")
    print(f"   model.load('{MODEL_FILE}')")
    print(f"   result = model.predict('ƒÉn qu·∫£ nh·ªõ')")
    print(f"   print(result)\n")


# ========== MAIN ==========
if __name__ == "__main__":
    train_and_test()