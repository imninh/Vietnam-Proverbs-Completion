"""
SCRIPT 2: L√ÄM S·∫†CH DATASET (PHI√äN B·∫¢N ƒê∆†N GI·∫¢N CHO RETRIEVAL)
File: 2_clean_data_simple.py

M·ª•c ƒë√≠ch: L√†m s·∫°ch dataset ƒë·ªÉ d√πng cho retrieval model
- X√≥a d√≤ng r·ªóng
- Chu·∫©n h√≥a Unicode
- X√≥a k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn thi·∫øt
- Gi·ªØ nguy√™n c√¢u ƒë·∫ßy ƒë·ªß (kh√¥ng t·∫°o input/target)

Ch·∫°y:
  python 2_clean_data_simple.py
"""

import re
import sys
import unicodedata
from pathlib import Path
from collections import Counter


class SimpleDatasetCleaner:
    """L√†m s·∫°ch dataset ca dao cho retrieval model"""
    
    def __init__(self):
        self.stats = {
            'original': 0,
            'empty_removed': 0,
            'too_short_removed': 0,
            'duplicate_removed': 0,
            'final': 0
        }
    
    def normalize_unicode(self, text):
        """Chu·∫©n h√≥a d·∫•u ti·∫øng Vi·ªát (NFC normalization)"""
        return unicodedata.normalize('NFC', text)
    
    def clean_text(self, text):
        """
        L√†m s·∫°ch text nh·∫π nh√†ng
        - X√≥a kho·∫£ng tr·∫Øng th·ª´a
        - Chu·∫©n h√≥a d·∫•u c√¢u
        - Gi·ªØ nguy√™n n·ªôi dung
        """
        # X√≥a BOM n·∫øu c√≥
        text = text.replace('\ufeff', '')
        
        # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
        text = ' '.join(text.split())
        
        # X√≥a d·∫•u c√°ch ƒë·∫ßu/cu·ªëi
        text = text.strip()
        
        return text
    
    def is_valid(self, text, min_words=3):
        """
        Ki·ªÉm tra c√¢u c√≥ h·ª£p l·ªá kh√¥ng
        - Kh√¥ng r·ªóng
        - √çt nh·∫•t min_words t·ª´
        - C√≥ √≠t nh·∫•t 50% ch·ªØ c√°i ti·∫øng Vi·ªát
        """
        if not text or len(text.strip()) == 0:
            return False
        
        words = text.split()
        if len(words) < min_words:
            return False
        
        # Ki·ªÉm tra t·ª∑ l·ªá ch·ªØ c√°i ti·∫øng Vi·ªát
        vietnamese_letters = re.findall(
            r'[a-z√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë]', 
            text.lower()
        )
        
        text_without_space = text.replace(' ', '').replace(',', '').replace('.', '')
        if len(text_without_space) == 0:
            return False
        
        letter_ratio = len(vietnamese_letters) / len(text_without_space)
        
        return letter_ratio >= 0.5
    
    def clean(self, input_file, output_file, min_words=3):
        """Pipeline ch√≠nh ƒë·ªÉ l√†m s·∫°ch dataset"""
        
        print("\n" + "="*70)
        print("üßπ L√ÄM S·∫†CH DATASET CA DAO (CHO RETRIEVAL)")
        print("="*70)
        
        # ƒê·ªçc file
        print(f"\nüìÇ ƒê·ªçc file: {input_file}")
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except FileNotFoundError:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {input_file}")
            return False
        
        self.stats['original'] = len(lines)
        print(f"‚úì ƒê·ªçc th√†nh c√¥ng {self.stats['original']:,} d√≤ng")
        
        # X·ª≠ l√Ω t·ª´ng d√≤ng
        print(f"\n{'‚îÄ'*70}")
        print("B∆Ø·ªöC 1: L√†m s·∫°ch v√† l·ªçc d·ªØ li·ªáu")
        
        cleaned_lines = []
        
        for line in lines:
            # B·ªè d√≤ng r·ªóng
            if not line.strip():
                self.stats['empty_removed'] += 1
                continue
            
            # Chu·∫©n h√≥a Unicode
            line = self.normalize_unicode(line)
            
            # L√†m s·∫°ch text
            line = self.clean_text(line)
            
            # Ki·ªÉm tra h·ª£p l·ªá
            if not self.is_valid(line, min_words):
                self.stats['too_short_removed'] += 1
                continue
            
            cleaned_lines.append(line)
        
        print(f"‚úì X√≥a {self.stats['empty_removed']:,} d√≤ng r·ªóng")
        print(f"‚úì X√≥a {self.stats['too_short_removed']:,} c√¢u qu√° ng·∫Øn ho·∫∑c kh√¥ng h·ª£p l·ªá")
        print(f"  C√≤n l·∫°i: {len(cleaned_lines):,} c√¢u")
        
        # X√≥a tr√πng l·∫∑p
        print(f"\n{'‚îÄ'*70}")
        print("B∆Ø·ªöC 2: X√≥a tr√πng l·∫∑p")
        
        seen = set()
        unique_lines = []
        
        for line in cleaned_lines:
            # So s√°nh kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng
            line_lower = line.lower()
            if line_lower not in seen:
                seen.add(line_lower)
                unique_lines.append(line)
            else:
                self.stats['duplicate_removed'] += 1
        
        print(f"‚úì X√≥a {self.stats['duplicate_removed']:,} c√¢u tr√πng l·∫∑p")
        print(f"  C√≤n l·∫°i: {len(unique_lines):,} c√¢u")
        
        self.stats['final'] = len(unique_lines)
        
        # L∆∞u k·∫øt qu·∫£
        print(f"\n{'‚îÄ'*70}")
        print("B∆Ø·ªöC 3: L∆∞u k·∫øt qu·∫£")
        
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for line in unique_lines:
                f.write(line + '\n')
        
        print(f"‚úì ƒê√£ l∆∞u: {output_file}")
        
        # B√°o c√°o
        self.print_report()
        
        # Hi·ªÉn th·ªã m·∫´u
        self.print_samples(unique_lines, n=10)
        
        return True
    
    def print_report(self):
        """In b√°o c√°o t·ªïng h·ª£p"""
        print(f"\n{'='*70}")
        print("üìä B√ÅO C√ÅO T·ªîNG H·ª¢P")
        print("="*70)
        
        print(f"\nüìà Th·ªëng k√™:")
        print(f"   D√≤ng g·ªëc:              {self.stats['original']:>6,}")
        print(f"   ‚îú‚îÄ X√≥a r·ªóng:           {self.stats['empty_removed']:>6,}")
        print(f"   ‚îú‚îÄ X√≥a kh√¥ng h·ª£p l·ªá:   {self.stats['too_short_removed']:>6,}")
        print(f"   ‚îî‚îÄ X√≥a tr√πng l·∫∑p:      {self.stats['duplicate_removed']:>6,}")
        print(f"   {'‚îÄ'*35}")
        print(f"   ‚úÖ C√≤n l·∫°i:            {self.stats['final']:>6,}")
        
        if self.stats['original'] > 0:
            retention = (self.stats['final'] / self.stats['original']) * 100
            print(f"\nüìä T·ª∑ l·ªá gi·ªØ l·∫°i: {retention:.1f}%")
            
            if retention < 70:
                print(f"   ‚ö†Ô∏è  L∆∞u √Ω: M·∫•t {100-retention:.1f}% d·ªØ li·ªáu")
            else:
                print(f"   ‚úÖ T·ªët! Gi·ªØ ƒë∆∞·ª£c ph·∫ßn l·ªõn d·ªØ li·ªáu")
    
    def print_samples(self, data, n=10):
        """Hi·ªÉn th·ªã m·∫´u d·ªØ li·ªáu sau khi l√†m s·∫°ch"""
        print(f"\n{'='*70}")
        print(f"üìù M·∫™U D·ªÆ LI·ªÜU SAU KHI L√ÄM S·∫†CH ({n} c√¢u ƒë·∫ßu)")
        print("="*70)
        
        for i, line in enumerate(data[:n], 1):
            word_count = len(line.split())
            # C·∫Øt ng·∫Øn n·∫øu c√¢u qu√° d√†i
            display = line if len(line) <= 70 else line[:67] + "..."
            print(f"   {i:2d}. {display} ({word_count} t·ª´)")


# ========== MAIN ==========
if __name__ == "__main__":
    # ƒê∆∞·ªùng d·∫´n - B·∫†N C·∫¶N S·ª¨A ƒê·ªîI CH·ªñ N√ÄY
# SAU (ph√π h·ª£p v·ªõi c·∫•u tr√∫c c·ªßa b·∫°n)
    BASE_DIR = Path(__file__).parent.parent  # backend/
    INPUT_FILE = BASE_DIR / "data" / "raw" / "dataset_normalized.txt"
    OUTPUT_FILE = BASE_DIR / "data" / "processed" / "cleaned_dataset.txt"
    
    # Tham s·ªë
    MIN_WORDS = 3  # T·ªëi thi·ªÉu 3 t·ª´ (c√¢u ng·∫Øn nh·∫•t trong ca dao)
    
    print("\nüöÄ B·∫ÆT ƒê·∫¶U L√ÄM S·∫†CH DATASET")
    print(f"üì• Input:  {INPUT_FILE}")
    print(f"üì§ Output: {OUTPUT_FILE}")
    print(f"‚öôÔ∏è  C·∫•u h√¨nh: T·ªëi thi·ªÉu {MIN_WORDS} t·ª´")
    
    # Ch·∫°y cleaner
    cleaner = SimpleDatasetCleaner()
    success = cleaner.clean(
        input_file=INPUT_FILE,
        output_file=OUTPUT_FILE,
        min_words=MIN_WORDS
    )
    
    if success:
        print(f"\n{'='*70}")
        print("‚úÖ HO√ÄN TH√ÄNH!")
        print("="*70)
        print(f"\nüìå B∆∞·ªõc ti·∫øp theo:")
        print(f"   1. Ki·ªÉm tra file {OUTPUT_FILE}")
        print(f"   2. Ch·∫°y 3_create_splits_simple.py ƒë·ªÉ t·∫°o train/val/test")
        print(f"   3. Train retrieval model\n")
    else:
        print(f"\n‚ùå L√†m s·∫°ch th·∫•t b·∫°i!")
        sys.exit(1)