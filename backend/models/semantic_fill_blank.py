"""
SEMANTIC FILL-IN-THE-BLANK MODEL
File: backend/models/semantic_fill_blank.py

Inspired by SP-GPT2 paper (2110.15723v1)
S·ª≠ d·ª•ng sentence embeddings ƒë·ªÉ t√¨m c√¢u c√≥ semantic similarity cao nh·∫•t
"""

from sentence_transformers import SentenceTransformer
import numpy as np
import json
import pickle
from pathlib import Path


class SemanticFillBlankModel:
    """
    Semantic-based Fill-in-Blank model
    
    C√°ch ho·∫°t ƒë·ªông (Inspired by SP-GPT2):
    1. Encode t·∫•t c·∫£ c√¢u trong database th√†nh semantic vectors
    2. Khi c√≥ input, encode input th√†nh vector
    3. T√≠nh cosine similarity gi·ªØa input v√† t·∫•t c·∫£ c√¢u
    4. Tr·∫£ v·ªÅ top-k c√¢u c√≥ semantic similarity cao nh·∫•t
    
    Kh√°c v·ªõi Retrieval (TF-IDF):
    - Retrieval: D·ª±a tr√™n t·ª´ kh√≥a (keyword matching)
    - Semantic: D·ª±a tr√™n √Ω nghƒ©a (semantic meaning)
    
    VD:
    - Input: "ƒÉn qu·∫£ nh·ªõ"
    - Retrieval c√≥ th·ªÉ match: "ƒÉn ch√°o ƒë√° b√°t" (c√≥ t·ª´ "ƒÉn")
    - Semantic s·∫Ω match: "u·ªëng n∆∞·ªõc nh·ªõ ngu·ªìn" (c√πng √Ω nghƒ©a bi·∫øt ∆°n)
    """
    
    def __init__(self, model_name='keepitreal/vietnamese-sbert'):
        """
        Args:
            model_name: Pre-trained sentence transformer model
                       'keepitreal/vietnamese-sbert' - Model ti·∫øng Vi·ªát t·ªët
        """
        print(f"üîÑ Loading SentenceTransformer: {model_name}")
        try:
            self.model = SentenceTransformer(model_name)
            print(f"‚úì Model loaded successfully")
        except Exception as e:
            print(f"‚ö†Ô∏è  L·ªói load model: {e}")
            print(f"üí° Fallback sang 'paraphrase-multilingual-MiniLM-L12-v2'")
            self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        self.database = []  # List of sentences
        self.embeddings = None  # Numpy array of embeddings
        self.is_trained = False
    
    def train(self, train_data):
        """
        Encode t·∫•t c·∫£ c√¢u trong database
        
        Args:
            train_data: List of dicts [{'full': '...', ...}]
        """
        print(f"\n{'‚îÄ'*60}")
        print(f"üîÑ TRAINING SEMANTIC FILL-BLANK MODEL")
        print(f"{'‚îÄ'*60}")
        
        # L·∫•y unique sentences
        seen = set()
        for item in train_data:
            sentence = item['full']
            if sentence not in seen:
                self.database.append(sentence)
                seen.add(sentence)
        
        print(f"üìä Database: {len(self.database)} sentences")
        
        # Encode t·∫•t c·∫£ c√¢u (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)
        print(f"üîÑ Encoding sentences (c√≥ th·ªÉ m·∫•t 1-2 ph√∫t)...")
        self.embeddings = self.model.encode(
            self.database,
            batch_size=32,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        print(f"‚úì Embeddings shape: {self.embeddings.shape}")
        print(f"‚úì Vector dimension: {self.embeddings.shape[1]}")
        
        self.is_trained = True
    
    def semantic_similarity(self, query_embedding, database_embeddings):
        """
        T√≠nh cosine similarity gi·ªØa query v√† database
        
        Returns:
            Numpy array of similarity scores
        """
        # Cosine similarity = dot product (v√¨ vectors ƒë√£ normalized)
        similarities = np.dot(database_embeddings, query_embedding)
        return similarities
    
    def predict_multiple(self, partial_input, top_k=3, min_similarity=0.3):
        """
        Tr·∫£ v·ªÅ top-k candidates d·ª±a tr√™n semantic similarity
        
        Args:
            partial_input: Input string
            top_k: S·ªë candidates
            min_similarity: Ng∆∞·ª°ng similarity t·ªëi thi·ªÉu
        
        Returns:
            List of dicts [{'text': '...', 'confidence': 0.9, 'model': 'semantic'}]
        """
        if not self.is_trained:
            raise ValueError("Model ch∆∞a ƒë∆∞·ª£c train!")
        
        # Encode query
        query_embedding = self.model.encode(
            partial_input.lower(),
            convert_to_numpy=True
        )
        
        # T√≠nh similarity
        similarities = self.semantic_similarity(query_embedding, self.embeddings)
        
        # L·∫•y top-k indices
        top_indices = np.argsort(similarities)[-top_k*2:][::-1]  # L·∫•y nhi·ªÅu h∆°n ƒë·ªÉ filter
        
        candidates = []
        
        for idx in top_indices:
            similarity = float(similarities[idx])
            
            # Filter theo threshold
            if similarity < min_similarity:
                continue
            
            # Map similarity ‚Üí confidence
            # Semantic similarity th∆∞·ªùng cao h∆°n TF-IDF
            confidence = min(0.99, similarity * 1.1)
            
            candidates.append({
                'text': self.database[idx],
                'confidence': round(confidence, 3),
                'model': 'semantic',
                'similarity': round(similarity, 3),
                'method': 'sentence_embedding'
            })
            
            if len(candidates) >= top_k:
                break
        
        # Fallback n·∫øu kh√¥ng t√¨m th·∫•y
        if not candidates:
            # L·∫•y top 1 d√π similarity th·∫•p
            best_idx = np.argmax(similarities)
            candidates = [{
                'text': self.database[best_idx],
                'confidence': 0.2,
                'model': 'semantic',
                'similarity': round(float(similarities[best_idx]), 3),
                'method': 'fallback'
            }]
        
        return candidates
    
    def predict(self, partial_input):
        """Wrapper tr·∫£ v·ªÅ 1 k·∫øt qu·∫£"""
        candidates = self.predict_multiple(partial_input, top_k=1)
        return candidates[0]['text'] if candidates else partial_input
    
    def evaluate(self, test_data):
        """ƒê√°nh gi√° model"""
        print(f"\n{'‚îÄ'*60}")
        print(f"üìä EVALUATING SEMANTIC FILL-BLANK MODEL")
        print(f"{'‚îÄ'*60}")
        
        exact_correct = 0
        top3_correct = 0
        total = len(test_data)
        similarities = []
        
        for item in test_data:
            candidates = self.predict_multiple(item['input'], top_k=3)
            
            # Exact match
            if candidates and candidates[0]['text'] == item['full']:
                exact_correct += 1
            
            # Top-3
            if candidates:
                top3_texts = [c['text'] for c in candidates]
                if item['full'] in top3_texts:
                    top3_correct += 1
            
            # Similarity
            if candidates:
                similarities.append(candidates[0]['similarity'])
        
        exact_acc = exact_correct / total if total > 0 else 0
        top3_acc = top3_correct / total if total > 0 else 0
        avg_sim = sum(similarities) / len(similarities) if similarities else 0
        
        print(f"Test samples: {total}")
        print(f"Exact matches: {exact_correct} ({exact_acc:.1%})")
        print(f"Top-3 matches: {top3_correct} ({top3_acc:.1%})")
        print(f"Avg similarity: {avg_sim:.3f}")
        
        return {
            'exact_accuracy': exact_acc,
            'top3_accuracy': top3_acc,
            'avg_similarity': avg_sim,
            'exact_correct': exact_correct,
            'top3_correct': top3_correct,
            'total': total
        }
    
    def save(self, file_path):
        """L∆∞u model"""
        data = {
            'database': self.database,
            'embeddings': self.embeddings,
            'model_name': 'keepitreal/vietnamese-sbert'
        }
        
        with open(file_path, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"‚úì Model saved to {file_path}")
    
    def load(self, file_path):
        """Load model"""
        with open(file_path, 'rb') as f:
            data = pickle.load(f)
        
        self.database = data['database']
        self.embeddings = data['embeddings']
        self.is_trained = True
        
        print(f"‚úì Model loaded from {file_path}")


# ========== TRAINING SCRIPT ==========
def train_semantic_model():
    """Script train v√† test"""
    
    print("\n" + "="*70)
    print("üöÄ SEMANTIC FILL-BLANK MODEL TRAINING")
    print("="*70)
    
    # Paths
    BASE_DIR = Path(__file__).parent.parent
    DATA_DIR = BASE_DIR / "data" / "processed"
    MODEL_DIR = BASE_DIR / "trained_models"
    
    MODEL_DIR.mkdir(exist_ok=True)
    
    # Load data
    print(f"\nüìÇ Loading data...")
    
    with open(DATA_DIR / "train.json", 'r', encoding='utf-8') as f:
        train_data = json.load(f)
    
    with open(DATA_DIR / "test.json", 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    print(f"‚úì Train: {len(train_data)} samples")
    print(f"‚úì Test:  {len(test_data)} samples")
    
    # Train
    model = SemanticFillBlankModel()
    model.train(train_data)
    
    # Test predictions
    print(f"\n{'‚îÄ'*60}")
    print("üß™ TEST PREDICTIONS")
    print(f"{'‚îÄ'*60}")
    
    test_inputs = [
        "ƒÉn",
        "ƒÉn qu·∫£",
        "ƒÉn qu·∫£ nh·ªõ",
        "c√≥ c√¥ng",
        "g·∫ßn m·ª±c",
        "h·ªçc th·∫ßy",
        "u·ªëng n∆∞·ªõc"  # Test semantic: gi·ªëng "ƒÉn qu·∫£ nh·ªõ" v·ªÅ √Ω nghƒ©a
    ]
    
    for inp in test_inputs:
        print(f"\nüìù Input: '{inp}'")
        candidates = model.predict_multiple(inp, top_k=3)
        
        for i, cand in enumerate(candidates, 1):
            print(f"   {i}. {cand['text']}")
            print(f"      üìä Confidence: {cand['confidence']:.1%} | Similarity: {cand['similarity']:.3f}")
    
    # Evaluate
    metrics = model.evaluate(test_data[:100])
    
    # Save
    model_path = MODEL_DIR / "semantic_model.pkl"
    model.save(model_path)
    
    print(f"\n{'='*70}")
    print("‚úÖ TRAINING COMPLETE!")
    print("="*70)
    print(f"\nüìä Summary:")
    print(f"   ‚Ä¢ Database size: {len(model.database):,} sentences")
    print(f"   ‚Ä¢ Embedding dim: {model.embeddings.shape[1]}")
    print(f"   ‚Ä¢ Exact accuracy: {metrics['exact_accuracy']:.1%}")
    print(f"   ‚Ä¢ Top-3 accuracy: {metrics['top3_accuracy']:.1%}")
    print(f"   ‚Ä¢ Avg similarity: {metrics['avg_similarity']:.3f}")
    print(f"   ‚Ä¢ Model saved: {model_path}")
    print()


# ========== MAIN ==========
if __name__ == "__main__":
    train_semantic_model()