# ğŸ‡»ğŸ‡³ Vietnamese Proverb & Folk Verse Completion System

<div align="center">

<!-- Badges -->
<img src="https://img.shields.io/badge/Python-3.8%2B-3572A5?style=flat-square&logo=python&logoColor=white" />
<img src="https://img.shields.io/badge/KenLM-5--gram-4CAF50?style=flat-square" />
<img src="https://img.shields.io/badge/Dataset-13%2C062%20verses-FF9800?style=flat-square" />
<img src="https://img.shields.io/badge/Accuracy-40.2%25%20exact%20|%2073.9%25%20similarity-2196F3?style=flat-square" />
<img src="https://img.shields.io/badge/License-MIT-gray?style=flat-square" />

<br/>

> **Ca dao vÃ  tá»¥c ngá»¯** lÃ  viÃªn ngoc vÃ´ giÃ¡ cá»§a vÄƒn hÃ³a Viá»‡t Nam â€” nhÆ°ng Ä‘ang dáº§n bá»‹ lÃ£ng quÃªn.  
> Project nÃ y dÃ¹ng NLP Ä‘á»ƒ giÃºp má»i ngÆ°á»i **gá»£i nhá»›, há»c thuá»™c, vÃ  báº£o tá»“n** di sáº£n vÄƒn hÃ³a truyá»n thá»‘ng.

</div>

---

## ğŸ“‘ Table of Contents

- [Overview](#overview)
- [How It Works](#how-it-works)
- [Architecture](#architecture)
- [Dataset](#dataset)
- [Models & Approaches](#models--approaches)
- [Results & Evaluation](#results--evaluation)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Web Deployment](#web-deployment)
- [Limitations & Future Work](#limitations--future-work)
- [Team & References](#team--references)

---

## Overview

Vietnamese folk poetry (**Ca Dao**) and proverbs (**Tá»¥c Ngá»¯**) carry centuries of cultural wisdom â€” from life lessons, moral values, to reflections on nature and human relationships. Yet in the digital era, younger generations are increasingly losing touch with these traditions.

**This project** builds an intelligent text completion system: given a partial Ca Dao or Tá»¥c Ngá»¯ verse, the system predicts and suggests the complete verse. It serves as:

| Purpose | Description |
|---|---|
| ğŸ“ **Educational Tool** | Helps students and learners recall and memorize traditional verses |
| ğŸ“š **Cultural Preservation** | Creates a digital, searchable repository of Vietnamese folk poetry |
| ğŸ¤– **NLP Research Baseline** | Establishes the first systematic benchmark for Vietnamese poetry completion |
| ğŸŒ **Public Accessibility** | Deployed as a web app â€” no specialized tools needed |

---

## How It Works

The core idea is simple: **type a few words you remember, and the system completes the verse for you.**

```
Input:  "cÃ´ng cha nhÆ° nÃºi"
Output: "CÃ´ng cha nhÆ° nÃºi ThÃ¡i SÆ¡n
         NghÄ©a máº¹ nhÆ° nÆ°á»›c trong nguá»“n cháº£y ra"

Input:  "gáº§n má»±c thÃ¬"
Output: "Gáº§n má»±c thÃ¬ Ä‘en, gáº§n Ä‘Ã¨n thÃ¬ sÃ¡ng"

Input:  "Äƒn quáº£ nhá»›"
Output: "Ä‚n quáº£ nhá»› káº» trá»“ng cÃ¢y"
```

The system supports **three completion scenarios**:

| Scenario | Input Position | Example |
|---|---|---|
| **Forward** | Beginning of verse | `"cÃ´ng cha"` â†’ completes the rest |
| **Backward** | End of verse | `"cháº£y ra"` â†’ finds the beginning |
| **Bidirectional** | Middle of verse | `"nÃºi thÃ¡i sÆ¡n"` â†’ reconstructs full verse |

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     User Input                          â”‚
â”‚              (partial Ca Dao / Tá»¥c Ngá»¯)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Vietnamese Preprocessing                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Unicode  â”‚â†’â”‚  PyVi Word  â”‚â†’â”‚  Special Tokens    â”‚  â”‚
â”‚  â”‚   NFC     â”‚ â”‚ Segmentationâ”‚ â”‚  <s> ... </s>      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼          â–¼              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  5-gram   â”‚ â”‚  TF-IDF  â”‚  â”‚ Ensemble   â”‚
    â”‚  KenLM    â”‚ â”‚ Retrievalâ”‚  â”‚ (30/70)    â”‚
    â”‚  + Beam   â”‚ â”‚  + BM25  â”‚  â”‚ N-gram +   â”‚
    â”‚  Search   â”‚ â”‚          â”‚  â”‚ Retrieval  â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚            â”‚              â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”˜              â”‚
                     â–¼                â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚    Ranked Suggestions       â”‚
          â”‚  (Score, Confidence, Text)  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Format & Display   â”‚
          â”‚  (Lá»¥c BÃ¡t layout)   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Dataset

### Collection & Curation

The dataset was sourced from authoritative Vietnamese cultural archives and digitized repositories, then manually verified for authenticity and linguistic correctness.

### Statistics

| Metric | Value |
|---|---|
| Total verified verses | **13,062** |
| Vocabulary size (unique tokens) | **6,763** |
| Total tokens | **97,305** |
| Dominant structure | Lá»¥c BÃ¡t (6â€“8 syllable lines) |
| Sentence length range | 6â€“14 words (majority) |

### Preprocessing Pipeline

```
Raw Text
    â”‚
    â–¼
â‘  Text Normalization          â€” Unicode NFC, lowercase, strip whitespace
    â”‚                            merge multi-line verses with "."
    â–¼
â‘¡ Word Segmentation (PyVi)    â€” "thÃ¡i sÆ¡n" â†’ "thÃ¡i_sÆ¡n"
    â”‚                            accuracy: 94.8% on 500 sample verses
    â–¼
â‘¢ Boundary Tagging            â€” prepend <s>, append </s>
    â”‚
    â–¼
â‘£ Quality Validation          â€” length filter (4â€“35 tokens),
    â”‚                            character validation, content check
    â–¼
train_data_seg.txt            â€” 13,062 clean, tokenized verses
```

> âš ï¸ **Critical**: Vietnamese tonal diacritics are **never** removed. `"ma"` (ghost) â‰  `"mÃ¡"` (mother) â‰  `"mÃ "` (but) â€” each tone carries distinct meaning.

---

## Models & Approaches

Three approaches were designed and compared. Each leverages different strengths of the data:

### 1. 5-gram Language Model (Primary) â€” KenLM + Bidirectional Beam Search

**The core model.** A 5-gram order was chosen because Vietnamese Ca Dao lines typically have 6â€“8 syllables, so a 4-word context window captures most of a half-line.

| Component | Details |
|---|---|
| **Toolkit** | KenLM (`lmplz`, `build_binary`) |
| **Smoothing** | Modified Kneser-Ney (handles unseen n-grams gracefully) |
| **Search** | Bidirectional Beam Search (beam width = 10 backward, 5 forward) |
| **Vocabulary Maps** | Pre-built `fwd_map` and `bwd_map` for O(1) context lookup |
| **Inference** | < 100ms per completion |

**Beam Search Flow:**
```
Phase 1 â€” Backward Expansion:
  seed_words â†’ bwd_map â†’ expand toward <s>
  (reconstruct the beginning of the verse)

Phase 2 â€” Forward Expansion:
  recovered prefix â†’ fwd_map + KenLM scoring â†’ expand toward </s>
  (complete the rest of the verse)

Post-processing:
  Deduplicate â†’ Re-rank by total log-probability â†’ Format output
```

**N-gram Statistics from training:**

| N-gram Order | Unique N-grams |
|---|---|
| 1-gram | 6,816 |
| 2-gram | 48,941 |
| 3-gram | 70,121 |
| 4-gram | 74,188 |
| 5-gram | 67,191 |

### 2. TF-IDF Retrieval Baseline

A **retrieval-based** approach that treats completion as an information retrieval problem.

- Vectorize all training verses using `TfidfVectorizer` (scikit-learn)
- Compute **cosine similarity** between input query and all stored verses
- Return top-K most similar complete verses

**Strengths:** Always returns valid, real verses from the corpus. Fast and simple.  
**Weaknesses:** Cannot generate novel completions â€” limited to lexical matching.

### 3. Ensemble Model (Best Performance)

Combines N-gram generation with retrieval for the best of both worlds.

```
â”Œâ”€ N-gram model  â†’ top-5 candidates  â”€â”€â”
â”‚                                       â–¼
â”‚                              Combined Scoring:
â”‚                              score = 0.3 Ã— ngram + 0.7 Ã— retrieval
â”‚                              + 0.15 bonus if both methods agree
â”‚                                       â”‚
â””â”€ TF-IDF retrieval â†’ top-5 results â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                              Final ranked output
```

| Model | Role |
|---|---|
| TF-IDF (70%) | Anchor â€” high precision for known patterns |
| N-gram (30%) | Flexibility â€” handles partial matches & variations |
| Agreement bonus (+0.15) | Confidence boost when both models agree |

---

## Results & Evaluation

### Test Setup

- **239,883 test cases** generated exhaustively from 1,306 held-out verses
- Three positional categories: Start / Mid / End
- Input lengths: 1 word â†’ 70% of verse length
- Metrics: **Exact Match Accuracy** and **Character-level Similarity** (`SequenceMatcher`)

### Overall Performance (5-gram Beam Search)

| Metric | Score |
|---|---|
| Exact Match Accuracy | **40.20%** |
| Average Similarity | **73.89%** |
| Similarity â‰¥ 70% | **74.0%** of all predictions |
| Similarity â‰¥ 90% | **45.3%** of all predictions |

### Performance by Input Position

| Position | Accuracy | Notes |
|---|---|---|
| **End** | Highest (~46.9%) | Verse endings are more formulaic & distinctive |
| **Start** | Mid-range | Forward generation benefits from longer context |
| **Mid** | Lowest | Requires both backward + forward expansion â€” hardest |

### Performance by Input Length

| Input Words | Behavior |
|---|---|
| 1 word | Low accuracy (high ambiguity) |
| 2â€“3 words | **Sweet spot** â€” accuracy 46â€“68% |
| 4+ words | Exact match drops (too specific), but similarity stays high |

### Similarity Distribution

| Similarity Range | % of Cases | Interpretation |
|---|---|---|
| 90â€“100% | 45.3% | Near-perfect or exact match |
| 70â€“89% | 28.7% | Semantically related, partial match |
| 50â€“69% | 16.2% | Some overlap, wrong verse |
| 0â€“49% | 9.8% | Completely wrong or failure |

> ğŸ’¡ **Key Insight:** The model rarely outputs nonsense. Even when not exactly correct, 74% of predictions are at least 70% similar to the ground truth â€” meaning most "failures" are near-misses.

---

## Installation

### Prerequisites

| Tool | Version |
|---|---|
| Python | 3.8+ |
| KenLM | latest |
| PyVi | â‰¥ 0.1.1 |
| scikit-learn | â‰¥ 0.24.0 |
| numpy | â‰¥ 1.19.0 |
| Flask | (for web app) |

### Setup

```bash
# 1. Clone the repository
git clone https://github.com/your-org/ca-dao-completion.git
cd ca-dao-completion

# 2. Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate      # Linux/Mac
# venv\Scripts\activate       # Windows

# 3. Install dependencies
pip install -r requirements.txt

# 4. Install KenLM (requires C++ compiler)
pip install kenlm
# If build fails, install cmake and a C++ compiler first:
# sudo apt install cmake g++        (Ubuntu)
# brew install cmake                (Mac)
```

### Train the Model

```bash
# Preprocess raw data â†’ tokenized training file
python ngram/preprocess.py

# Train 5-gram KenLM model
python ngram/train.py
# Output: checkpoint/model.bin
```

## Web Deployment: [https://vietnam-proverbs-completion-7f5b.vercel.app]

```

### Deployment Checklist

- [ ] Use `model.bin` (binary) â€” loads in milliseconds vs. seconds for ARPA
- [ ] Enable caching for TF-IDF vectors (computed once at startup)
- [ ] Set `FLASK_ENV=production`
- [ ] Response target: **< 100ms** per completion

---

## Limitations & Future Work

### Current Limitations

The following are known constraints of the current system â€” each represents a concrete direction for future improvement.

| # | Area | Limitation |
|---|---|---|
| 1 | ğŸªŸ **Context Window** | 5-gram captures only 4 words of context. Long-range poetic structure (e.g. thematic coherence across a full verse) may be missed. |
| 2 | ğŸ“¦ **Dataset Size** | 13,062 verses is sufficient for N-gram modeling, but too small to fine-tune neural models like PhoBERT without overfitting. |
| 3 | ğŸ§  **Semantic Understanding** | The model is purely statistical â€” it matches surface-level word patterns, not meaning. It cannot reason about theme or emotion. |
| 4 | ğŸ—ºï¸ **Regional Variation** | The corpus may over-represent certain regions or historical periods, under-representing rarer Ca Dao dialects. |
| 5 | ğŸ“Š **Evaluation** | All metrics are automated (exact match, similarity). No expert literary evaluation has been conducted yet. |

---

### Roadmap

```
NOW â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º SHORT-TERM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º MEDIUM-TERM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º LONG-TERM
                    (next 3â€“6 mo)          (6â€“12 mo)               (12+ mo)

  âœ… 5-gram          ğŸ“¦ Expand dataset      ğŸ¤– Fine-tune            ğŸ¨ Poetry
  âœ… TF-IDF          ğŸ”§ VnCoreNLP           PhoBERT / ViT5         generation
  âœ… Ensemble        ğŸ“ Confidence          ğŸ“ Lá»¥c BÃ¡t meter       ğŸ® Gamified
  âœ… Web app         ğŸ”„ User feedback       ğŸ”— Multi-task          ğŸŒ Multi-
  âœ… Evaluation                             learning               cultural
```

**Short-term** â€” foundation improvements

- [ ] Expand dataset to **50,000+ verses** from additional cultural archives (target: +10â€“15% accuracy)
- [ ] Swap in `VnCoreNLP` for higher-accuracy Vietnamese word segmentation (currently PyVi at 94.8%)
- [ ] Add confidence calibration via temperature scaling so scores better reflect true accuracy
- [ ] Wire up a user feedback loop in the web app â€” let users correct wrong completions

**Medium-term** â€” move toward neural & structural modeling

- [ ] Fine-tune **PhoBERT** or **ViT5** on the Ca Dao corpus once dataset is large enough
- [ ] Explicitly model **Lá»¥c BÃ¡t** meter constraints (6â€“8 syllable structure + rhyme scheme) as a generation filter
- [ ] Explore multi-task learning: joint training on completion, verse classification, and paraphrasing

**Long-term** â€” beyond completion

- [ ] Controlled **poetry generation** â€” create brand-new Ca Dao matching a user-specified theme or structure
- [ ] **Multimodal** inputs: generate verses inspired by images (e.g. Vietnamese landscapes) or pair with traditional melodies
- [ ] Build a **gamified learning platform** with progressive difficulty, quizzes, and streak tracking
- [ ] Expand into a cross-cultural poetry system bridging Vietnamese, Chinese, and Japanese traditions

---

## Team & References

### ğŸ‘¥ Project Team â€” Group 16

| Name | Student ID | Role |
|---|---|---|
| VÅ© XuÃ¢n Anh | 20233832 | 
| ÄÃ o Há»¯u Mao | 20233865 | 
| Tráº§n Tháº¿ Ninh | 20233873 | 
| LÃª Thi Tháº£o | 20233877 | 

| | |
|---|---|
| **Supervisor** | Dr. Äá»— Thá»‹ Ngá»c Diá»‡p |
| **Institution** | Hanoi University of Science and Technology |
| **Department** | School of Electrical and Electronic Engineering |
| **Course** | Natural Language Processing â€” Class 161838 |

---

### ğŸ“š Key References

1. Jurafsky, D. & Martin, J. H. (2023). *Speech and Language Processing*, 3rd ed. â€” Pearson.
2. Nguyen, P. T. & Nguyen, L. M. (2020). *PhoBERT: Pre-trained language models for Vietnamese.* Findings of EMNLP 2020.
3. Heafield, K. (2011). *KenLM: Faster and Smaller Language Model Queries.* â€” WMT 2011.
4. Kneser, R. & Ney, H. (1995). *Improved backing-off techniques for estimating n-gram probabilities.*
5. Vu, M. H., Hoang, A. C. & Nguyen, T. T. (2021). *Vietnamese Natural Language Processing: A Survey.* arXiv:2103.01331.
6. Vaswani, A. et al. (2017). *Attention is all you need.* NeurIPS 2017.

---

<div align="center">

> *"Má»™t cÃ¢y lÃ m cháº£ nÃªn há»“i,*
> *Ba cÃ¢y chá»¥m láº¡i máº¥y Ä‘á»i cÃ²n xanhğŸŒ¿"*


</div>
